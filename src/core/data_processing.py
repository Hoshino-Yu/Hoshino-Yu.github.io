#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

class ComprehensiveDataProcessor:
    def __init__(self, output_dir='.', config_file=None):
        self.output_dir = output_dir
        self.config_file = config_file
        self.merged_data = None
        self.final_dataset = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.encoded_features = []
        self.processing_log = []
        self.feature_importance = None
        
        self.files_config = {
            'ä¼ä¸šåŸºæœ¬ä¿¡æ¯': '../åŸå§‹æ•°æ®/01_ä¼ä¸šåŸºæœ¬ä¿¡æ¯.xlsx',
            'ä¿¡è´·ç”³è¯·æ•°æ®': '../åŸå§‹æ•°æ®/02_ä¿¡è´·ç”³è¯·æ•°æ®.xlsx', 
            'è´¢åŠ¡æŠ¥è¡¨æ•°æ®': '../åŸå§‹æ•°æ®/03_è´¢åŠ¡æŠ¥è¡¨æ•°æ®.xlsx',
            'ç¨åŠ¡è®°å½•': '../åŸå§‹æ•°æ®/04_ç¨åŠ¡è®°å½•.xlsx',
            'é“¶è¡Œæµæ°´': '../åŸå§‹æ•°æ®/05_é“¶è¡Œæµæ°´.xlsx',
            'ä¼ä¸šä¸»ä¿¡ç”¨æ•°æ®': '../åŸå§‹æ•°æ®/06_ä¼ä¸šä¸»ä¿¡ç”¨æ•°æ®.xlsx',
            'è¡Œä¸šç»è¥çŠ¶å†µ': '../åŸå§‹æ•°æ®/07_è¡Œä¸šç»è¥çŠ¶å†µ.xlsx',
            'ä¼ä¸šå¾ä¿¡æŠ¥å‘Š': '../åŸå§‹æ•°æ®/08_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š.xlsx',
            'ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ': '../åŸå§‹æ•°æ®/09_ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ.xlsx',
            'ä¼ä¸šç»è¥é£é™©è¯„ä¼°': '../åŸå§‹æ•°æ®/10_ä¼ä¸šç»è¥é£é™©è¯„ä¼°.xlsx',
            'åˆå¹¶ä¿¡è´·æ•°æ®': '../åŸå§‹æ•°æ®/ä¸­å°ä¼ä¸šåˆå¹¶ä¿¡è´·æ•°æ®.xlsx'
        }
        
        self.features_to_remove = [
            'Unnamed: 0', 'ä¼ä¸šID', 'ä¼ä¸šåç§°', 'ä¼ä¸šæ³•äºº', 'ä¼ä¸šä¸»å§“å',
            'ç”³è¯·æ—¥æœŸ', 'æœ€è¿‘ä¸€æ¬¡ä¿¡ç”¨æŸ¥è¯¢æ—¶é—´', 'æœˆä»½', 'å¹´ä»½'
        ]
        
        self.custom_encoding_rules = {
            'binary': {
                'æ˜¯': 1, 'å¦': 0,
                'æœ‰': 1, 'æ— ': 0,
                'True': 1, 'False': 0,
                'ç”·': 1, 'å¥³': 0
            },
            'level_high_low': {
                'é«˜': 3, 'ä¸­': 2, 'ä½': 1
            },
            'credit_grade': {
                'A': 4, 'B': 3, 'C': 2, 'M': 1
            },
            'education': {
                'åšå£«': 5, 'ç¡•å£«': 4, 'æœ¬ç§‘': 3, 'ä¸“ç§‘': 2, 'é«˜ä¸­åŠä»¥ä¸‹': 1
            }
        }
        
        self.categorical_features = [
            'æ‰€åœ¨åœ°åŒº', 'æ‰€å±è¡Œä¸š', 'çº³ç¨ä¿¡ç”¨ç­‰çº§', 'è¡Œä¸šé£é™©ç­‰çº§',
            'ä¾›åº”é“¾ç¨³å®šæ€§è¯„çº§', 'é£é™©ç­‰çº§', 'æ‹…ä¿æ–¹å¼', 'ä¼ä¸šç±»å‹',
            'æ˜¯å¦é«˜æ–°æŠ€æœ¯ä¼ä¸š', 'æ˜¯å¦æœ‰å¤±ä¿¡è®°å½•', 'æ˜¯å¦æœ‰ä¸è‰¯è®°å½•',
            'æ˜¯å¦æ¶‰è¯‰', 'è¡Œä¸šç«äº‰ç¨‹åº¦', 'æ•™è‚²ç¨‹åº¦', 'èŒä¸šèƒŒæ™¯',
            'ç”³è¯·æœºæ„', 'è´·æ¬¾ç”¨é€”', 'æ˜¯å¦æœ‰æœªç»“æ¸…è´·æ¬¾'
        ]
        
        self.chinese_name_mapping = {
            'æ³¨å†Œèµ„æœ¬(ä¸‡å…ƒ)_ä¼ä¸šåŸºæœ¬': 'æ³¨å†Œèµ„æœ¬',
            'æˆç«‹å¹´é™_ä¼ä¸šåŸºæœ¬': 'æˆç«‹å¹´é™',
            'å‘˜å·¥äººæ•°_ä¼ä¸šåŸºæœ¬': 'å‘˜å·¥äººæ•°',
            'æ˜¯å¦é«˜æ–°æŠ€æœ¯ä¼ä¸š_ä¼ä¸šåŸºæœ¬': 'é«˜æ–°æŠ€æœ¯ä¼ä¸š',
            'ä¼ä¸šç±»å‹_ä¼ä¸šåŸºæœ¬': 'ä¼ä¸šç±»å‹',
            'æ‰€åœ¨åœ°åŒº_ä¼ä¸šåŸºæœ¬': 'æ‰€åœ¨åœ°åŒº',
            'æ‰€å±è¡Œä¸š_ä¼ä¸šåŸºæœ¬': 'æ‰€å±è¡Œä¸š',
            'æ€»èµ„äº§(ä¸‡å…ƒ)_è´¢åŠ¡æŠ¥è¡¨': 'æ€»èµ„äº§',
            'å‡€èµ„äº§(ä¸‡å…ƒ)_è´¢åŠ¡æŠ¥è¡¨': 'å‡€èµ„äº§',
            'è¥ä¸šæ”¶å…¥(ä¸‡å…ƒ)_è´¢åŠ¡æŠ¥è¡¨': 'è¥ä¸šæ”¶å…¥',
            'å‡€åˆ©æ¶¦(ä¸‡å…ƒ)_è´¢åŠ¡æŠ¥è¡¨': 'å‡€åˆ©æ¶¦',
            'èµ„äº§è´Ÿå€ºç‡_è´¢åŠ¡æŠ¥è¡¨': 'èµ„äº§è´Ÿå€ºç‡',
            'æµåŠ¨æ¯”ç‡_è´¢åŠ¡æŠ¥è¡¨': 'æµåŠ¨æ¯”ç‡',
            'é€ŸåŠ¨æ¯”ç‡_è´¢åŠ¡æŠ¥è¡¨': 'é€ŸåŠ¨æ¯”ç‡',
            'å‡€èµ„äº§æ”¶ç›Šç‡_è´¢åŠ¡æŠ¥è¡¨': 'å‡€èµ„äº§æ”¶ç›Šç‡',
            'æ€»èµ„äº§å‘¨è½¬ç‡_è´¢åŠ¡æŠ¥è¡¨': 'æ€»èµ„äº§å‘¨è½¬ç‡',
            'ç”³è¯·é‡‘é¢(ä¸‡å…ƒ)_ä¿¡è´·ç”³è¯·': 'ç”³è¯·é‡‘é¢',
            'è´·æ¬¾æœŸé™(æœˆ)_ä¿¡è´·ç”³è¯·': 'è´·æ¬¾æœŸé™',
            'æ‹…ä¿æ–¹å¼_ä¿¡è´·ç”³è¯·': 'æ‹…ä¿æ–¹å¼',
            'è´·æ¬¾ç”¨é€”_ä¿¡è´·ç”³è¯·': 'è´·æ¬¾ç”¨é€”',
            'ç”³è¯·æœºæ„_ä¿¡è´·ç”³è¯·': 'ç”³è¯·æœºæ„',
            'æ˜¯å¦æœ‰æœªç»“æ¸…è´·æ¬¾_ä¿¡è´·ç”³è¯·': 'æœ‰æœªç»“æ¸…è´·æ¬¾',
            'ä¼ä¸šä¿¡ç”¨è¯„åˆ†_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š': 'ä¼ä¸šä¿¡ç”¨è¯„åˆ†',
            'è´·æ¬¾é€¾æœŸæ¬¡æ•°_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š': 'è´·æ¬¾é€¾æœŸæ¬¡æ•°',
            'æœ€é•¿é€¾æœŸå¤©æ•°_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š': 'æœ€é•¿é€¾æœŸå¤©æ•°',
            'æ˜¯å¦æœ‰ä¸è‰¯è®°å½•_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š': 'æœ‰ä¸è‰¯è®°å½•',
            'æ˜¯å¦æ¶‰è¯‰_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š': 'æ¶‰è¯‰æƒ…å†µ',
            'çº³ç¨ä¿¡ç”¨ç­‰çº§_ç¨åŠ¡è®°å½•': 'çº³ç¨ä¿¡ç”¨ç­‰çº§',
            'ä¸ªäººæœˆæ”¶å…¥(ä¸‡å…ƒ)_ä¼ä¸šä¸»ä¿¡ç”¨': 'ä¼ä¸šä¸»æœˆæ”¶å…¥',
            'ä¸ªäººèµ„äº§(ä¸‡å…ƒ)_ä¼ä¸šä¸»ä¿¡ç”¨': 'ä¼ä¸šä¸»èµ„äº§',
            'æ•™è‚²ç¨‹åº¦_ä¼ä¸šä¸»ä¿¡ç”¨': 'æ•™è‚²ç¨‹åº¦',
            'èŒä¸šèƒŒæ™¯_ä¼ä¸šä¸»ä¿¡ç”¨': 'èŒä¸šèƒŒæ™¯',
            'æ˜¯å¦æœ‰å¤±ä¿¡è®°å½•_ä¼ä¸šä¸»ä¿¡ç”¨': 'æœ‰å¤±ä¿¡è®°å½•',
            'ä¼ä¸šä¸»ä¿¡ç”¨è¯„åˆ†_ä¼ä¸šä¸»ä¿¡ç”¨': 'ä¼ä¸šä¸»ä¿¡ç”¨è¯„åˆ†',
            'è¡Œä¸šå¢é•¿ç‡_è¡Œä¸šç»è¥çŠ¶å†µ': 'è¡Œä¸šå¢é•¿ç‡',
            'è¡Œä¸šç«äº‰ç¨‹åº¦_è¡Œä¸šç»è¥çŠ¶å†µ': 'è¡Œä¸šç«äº‰ç¨‹åº¦',
            'è¡Œä¸šé£é™©ç­‰çº§_è¡Œä¸šç»è¥çŠ¶å†µ': 'è¡Œä¸šé£é™©ç­‰çº§',
            'å¸‚åœºé£é™©_ä¼ä¸šç»è¥é£é™©è¯„ä¼°': 'å¸‚åœºé£é™©',
            'ç»è¥é£é™©_ä¼ä¸šç»è¥é£é™©è¯„ä¼°': 'ç»è¥é£é™©',
            'è´¢åŠ¡é£é™©_ä¼ä¸šç»è¥é£é™©è¯„ä¼°': 'è´¢åŠ¡é£é™©',
            'é£é™©ç­‰çº§_ä¼ä¸šç»è¥é£é™©è¯„ä¼°': 'é£é™©ç­‰çº§',
            'æœˆå‡æ”¶å…¥(ä¸‡å…ƒ)_é“¶è¡Œæµæ°´': 'æœˆå‡æ”¶å…¥',
            'æœˆå‡æ”¯å‡º(ä¸‡å…ƒ)_é“¶è¡Œæµæ°´': 'æœˆå‡æ”¯å‡º',
            'è´¦æˆ·ä½™é¢(ä¸‡å…ƒ)_é“¶è¡Œæµæ°´': 'è´¦æˆ·ä½™é¢',
            'æµæ°´ç¨³å®šæ€§_é“¶è¡Œæµæ°´': 'æµæ°´ç¨³å®šæ€§',
            'ä¾›åº”é“¾ç¨³å®šæ€§è¯„çº§_ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ': 'ä¾›åº”é“¾ç¨³å®šæ€§',
            'ä¸»è¦å®¢æˆ·æ•°é‡_ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ': 'ä¸»è¦å®¢æˆ·æ•°é‡',
            'ä¸»è¦ä¾›åº”å•†æ•°é‡_ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ': 'ä¸»è¦ä¾›åº”å•†æ•°é‡',
            'æ˜¯å¦æœ‰é€¾æœŸ': 'æ˜¯å¦é€¾æœŸ'
        }
        
        self._log_step("æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _log_step(self, message, details=None):
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = {
            'timestamp': timestamp,
            'message': message,
            'details': details
        }
        self.processing_log.append(log_entry)
        print(f"[{timestamp}] {message}")
        if details:
            print(f"  è¯¦æƒ…: {details}")
    
    def load_and_aggregate_data(self):
        self._log_step("=== å¼€å§‹æ•°æ®åŠ è½½å’Œèšåˆ ===")
        
        main_file = self.files_config['åˆå¹¶ä¿¡è´·æ•°æ®']
        if os.path.exists(main_file):
            self.merged_data = pd.read_excel(main_file)
            self._log_step(f"åŠ è½½ä¸»æ•°æ®é›†: {main_file}", f"å½¢çŠ¶: {self.merged_data.shape}")
            self.merged_data.columns = [col.replace('\ufeff', '') for col in self.merged_data.columns]
        else:
            raise FileNotFoundError(f"ä¸»æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {main_file}")
        
        loaded_files = []
        for name, filename in self.files_config.items():
            if name == 'åˆå¹¶ä¿¡è´·æ•°æ®':
                continue
                
            if os.path.exists(filename):
                try:
                    df = pd.read_excel(filename)
                    self._log_step(f"åŠ è½½æ•°æ®é›†: {filename}")
                    
                    id_col = self._find_id_column(df)
                    if id_col:
                        unique_ids = df[id_col].nunique()
                        total_rows = len(df)
                        
                        if total_rows > unique_ids:
                            df_aggregated = self._aggregate_multiple_records(df, id_col, name)
                            self._log_step(f"èšåˆå¤šæ¡è®°å½•", f"{total_rows} â†’ {len(df_aggregated)} è¡Œ")
                        else:
                            df_aggregated = df
                        
                        suffix = f"_{name.replace('æ•°æ®', '').replace('ä¿¡æ¯', '')}"
                        df_renamed = df_aggregated.rename(columns={col: f"{col}{suffix}" if col != id_col else col 
                                                          for col in df_aggregated.columns})
                        
                        self.merged_data = pd.merge(self.merged_data, df_renamed, 
                                                  left_on='ä¼ä¸šID', right_on=id_col, 
                                                  how='left', suffixes=('', suffix))
                        loaded_files.append(filename)
                        
                except Exception as e:
                    self._log_step(f"åŠ è½½å¤±è´¥: {filename}", str(e))
            else:
                self._log_step(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        
        self._log_step("æ•°æ®åŠ è½½å®Œæˆ", f"æœ€ç»ˆå½¢çŠ¶: {self.merged_data.shape}, æˆåŠŸåŠ è½½: {len(loaded_files)} ä¸ªæ–‡ä»¶")
        return self.merged_data
    
    def _find_id_column(self, df):
        possible_id_cols = ['ä¼ä¸šID', 'ID', 'id', 'ç¼–å·']
        for col in df.columns:
            if any(id_name in col for id_name in possible_id_cols):
                return col
        return None
    
    def _aggregate_multiple_records(self, df, id_col, dataset_name):
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        text_cols = df.select_dtypes(include=['object']).columns.tolist()
        
        if id_col in numeric_cols:
            numeric_cols.remove(id_col)
        if id_col in text_cols:
            text_cols.remove(id_col)
        
        agg_dict = {}
        
        for col in numeric_cols:
            if any(keyword in col for keyword in ['é‡‘é¢', 'æ”¶å…¥', 'æ”¯å‡º', 'ä½™é¢', 'èµ„äº§', 'è´Ÿå€º', 'åˆ©æ¶¦']):
                if 'æµæ°´' in dataset_name or 'é“¶è¡Œ' in dataset_name:
                    agg_dict[col] = 'sum'
                else:
                    agg_dict[col] = 'mean'
            elif any(keyword in col for keyword in ['ç‡', 'æ¯”ä¾‹', 'ç™¾åˆ†æ¯”']):
                agg_dict[col] = 'mean'
            elif any(keyword in col for keyword in ['æ¬¡æ•°', 'ç¬”æ•°', 'æ•°é‡']):
                agg_dict[col] = 'sum'
            else:
                agg_dict[col] = 'mean'
        
        for col in text_cols:
            agg_dict[col] = 'first'
        
        df_agg = df.groupby(id_col).agg(agg_dict).reset_index()
        return df_agg
    
    def clean_features(self):
        self._log_step("=== ç‰¹å¾æ¸…ç† ===")
        
        initial_cols = self.merged_data.shape[1]
        
        cols_to_drop = []
        for col in self.features_to_remove:
            matching_cols = [c for c in self.merged_data.columns if col in c]
            cols_to_drop.extend(matching_cols)
        
        cols_to_drop = list(set(cols_to_drop))
        
        if cols_to_drop:
            self.merged_data = self.merged_data.drop(columns=cols_to_drop)
            self._log_step(f"åˆ é™¤æ— ç”¨ç‰¹å¾", f"åˆ é™¤äº† {len(cols_to_drop)} ä¸ªç‰¹å¾")
        
        duplicate_cols = []
        for i, col1 in enumerate(self.merged_data.columns):
            for col2 in self.merged_data.columns[i+1:]:
                if self.merged_data[col1].equals(self.merged_data[col2]):
                    duplicate_cols.append(col2)
        
        if duplicate_cols:
            self.merged_data = self.merged_data.drop(columns=duplicate_cols)
            self._log_step(f"åˆ é™¤é‡å¤åˆ—", f"åˆ é™¤äº† {len(duplicate_cols)} ä¸ªé‡å¤åˆ—")
        
        constant_cols = []
        for col in self.merged_data.columns:
            if self.merged_data[col].nunique() <= 1:
                constant_cols.append(col)
        
        if constant_cols:
            self.merged_data = self.merged_data.drop(columns=constant_cols)
            self._log_step(f"åˆ é™¤å¸¸æ•°åˆ—", f"åˆ é™¤äº† {len(constant_cols)} ä¸ªå¸¸æ•°åˆ—")
        
        final_cols = self.merged_data.shape[1]
        self._log_step("ç‰¹å¾æ¸…ç†å®Œæˆ", f"{initial_cols} â†’ {final_cols} åˆ—")
        
        return self.merged_data
    
    def handle_missing_values(self):
        self._log_step("=== å¤„ç†ç¼ºå¤±å€¼ ===")
        
        missing_summary = self.merged_data.isnull().sum()
        missing_cols = missing_summary[missing_summary > 0]
        
        if len(missing_cols) > 0:
            self._log_step(f"å‘ç°ç¼ºå¤±å€¼", f"{len(missing_cols)} åˆ—æœ‰ç¼ºå¤±å€¼")
            for col, count in missing_cols.items():
                if self.merged_data[col].dtype == 'object':
                    mode_val = self.merged_data[col].mode()
                    if len(mode_val) > 0:
                        self.merged_data[col].fillna(mode_val[0], inplace=True)
                    else:
                        self.merged_data[col].fillna('æœªçŸ¥', inplace=True)
                else:
                    median_val = self.merged_data[col].median()
                    self.merged_data[col].fillna(median_val, inplace=True)
        else:
            self._log_step("ç¼ºå¤±å€¼æ£€æŸ¥", "æ— ç¼ºå¤±å€¼")
        
        return self.merged_data
    
    def encode_categorical_features(self):
        self._log_step("=== åˆ†ç±»ç‰¹å¾æ ‡ç­¾ç¼–ç  ===")
        
        encoded_count = 0
        self.encoded_features = []
        encoding_details = {}
        
        for feature_name in self.categorical_features:
            matching_cols = [col for col in self.merged_data.columns 
                           if feature_name in col and self.merged_data[col].dtype == 'object']
            
            for col in matching_cols:
                unique_values = self.merged_data[col].unique()
                encoded_values = self._apply_custom_encoding(col, unique_values)
                
                if encoded_values:
                    self.merged_data[col] = self.merged_data[col].map(encoded_values)
                    encoding_details[col] = {'type': 'custom', 'mapping': encoded_values}
                else:
                    le = LabelEncoder()
                    self.merged_data[col] = le.fit_transform(self.merged_data[col])
                    self.label_encoders[col] = le
                    encoding_details[col] = {'type': 'label', 'classes': le.classes_.tolist()}
                
                self.encoded_features.append(col)
                encoded_count += 1
        
        self._log_step("åˆ†ç±»ç‰¹å¾ç¼–ç å®Œæˆ", f"ç¼–ç äº† {encoded_count} ä¸ªåˆ†ç±»ç‰¹å¾")
        return self.merged_data
    
    def _apply_custom_encoding(self, col_name, unique_values):
        if len(unique_values) == 2:
            for val in unique_values:
                if val in self.custom_encoding_rules['binary']:
                    encoding_map = {}
                    for val in unique_values:
                        if val in self.custom_encoding_rules['binary']:
                            encoding_map[val] = self.custom_encoding_rules['binary'][val]
                        else:
                            existing_vals = [v for v in unique_values if v in self.custom_encoding_rules['binary']]
                            if existing_vals:
                                existing_code = self.custom_encoding_rules['binary'][existing_vals[0]]
                                encoding_map[val] = 1 - existing_code
                    return encoding_map
        
        if 'ä¿¡ç”¨' in col_name or 'ç­‰çº§' in col_name:
            if all(val in self.custom_encoding_rules['credit_grade'] for val in unique_values):
                return {val: self.custom_encoding_rules['credit_grade'][val] for val in unique_values}
        
        if 'æ•™è‚²' in col_name:
            if all(val in self.custom_encoding_rules['education'] for val in unique_values):
                return {val: self.custom_encoding_rules['education'][val] for val in unique_values}
        
        if all(val in self.custom_encoding_rules['level_high_low'] for val in unique_values):
            return {val: self.custom_encoding_rules['level_high_low'][val] for val in unique_values}
        
        return None
    
    def feature_selection(self, importance_threshold=0.001):
        self._log_step("=== ç‰¹å¾é€‰æ‹© ===")
        
        target_col = 'æ˜¯å¦æœ‰é€¾æœŸ'
        if target_col not in self.merged_data.columns:
            self._log_step("è­¦å‘Š", "æœªæ‰¾åˆ°ç›®æ ‡å˜é‡ï¼Œè·³è¿‡ç‰¹å¾é€‰æ‹©")
            return self.merged_data
        
        X = self.merged_data.drop(columns=[target_col])
        y = self.merged_data[target_col]
        
        original_features = X.shape[1]
        
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        self.feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        selected_features = self.feature_importance[
            self.feature_importance['importance'] > importance_threshold
        ]['feature'].tolist()
        
        self.merged_data = self.merged_data[selected_features + [target_col]]
        
        self._log_step("ç‰¹å¾é€‰æ‹©å®Œæˆ", 
                      f"é˜ˆå€¼: {importance_threshold}, {original_features} â†’ {len(selected_features)} ç‰¹å¾")
        
        return self.merged_data
    
    def optimize_standardization(self):
        self._log_step("=== ä¼˜åŒ–æ ‡å‡†åŒ–å¤„ç† ===")
        
        target_col = 'æ˜¯å¦æœ‰é€¾æœŸ'
        if target_col in self.merged_data.columns:
            features_to_standardize = []
            encoded_features_found = []
            
            for col in self.merged_data.columns:
                if col == target_col:
                    continue
                
                is_encoded = any(encoded_col in col for encoded_col in self.encoded_features)
                
                if not is_encoded and self.merged_data[col].dtype in ['int64', 'float64']:
                    features_to_standardize.append(col)
                else:
                    encoded_features_found.append(col)
            
            if features_to_standardize:
                self.merged_data[features_to_standardize] = self.scaler.fit_transform(
                    self.merged_data[features_to_standardize])
                self._log_step("è¿ç»­ç‰¹å¾æ ‡å‡†åŒ–", f"æ ‡å‡†åŒ–äº† {len(features_to_standardize)} ä¸ªè¿ç»­ç‰¹å¾")
            
            if encoded_features_found:
                self._log_step("ç¼–ç ç‰¹å¾ä¿æŒåŸå€¼", f"{len(encoded_features_found)} ä¸ªç¼–ç ç‰¹å¾æœªæ ‡å‡†åŒ–")
        
        return self.merged_data
    
    def simplify_chinese_names(self):
        self._log_step("=== ç®€åŒ–ä¸­æ–‡åˆ—å ===")
        
        new_column_mapping = {}
        
        for old_col in self.merged_data.columns:
            if old_col in self.chinese_name_mapping:
                new_column_mapping[old_col] = self.chinese_name_mapping[old_col]
            else:
                new_col = old_col
                
                suffixes_to_remove = [
                    '_ä¼ä¸šåŸºæœ¬ä¿¡æ¯', '_è´¢åŠ¡æŠ¥è¡¨æ•°æ®', '_ä¿¡è´·ç”³è¯·æ•°æ®', '_ç¨åŠ¡è®°å½•',
                    '_é“¶è¡Œæµæ°´', '_ä¼ä¸šä¸»ä¿¡ç”¨æ•°æ®', '_è¡Œä¸šç»è¥çŠ¶å†µ', '_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š',
                    '_ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ', '_ä¼ä¸šç»è¥é£é™©è¯„ä¼°', '_ä¼ä¸šåŸºæœ¬', '_è´¢åŠ¡æŠ¥è¡¨',
                    '_ä¿¡è´·ç”³è¯·', '_ä¼ä¸šä¸»ä¿¡ç”¨', '_è¡Œä¸šç»è¥', '_ä¼ä¸šå¾ä¿¡', '_ä¸Šä¸‹æ¸¸åˆä½œ',
                    '_ä¼ä¸šç»è¥é£é™©', '_é£é™©è¯„ä¼°'
                ]
                
                for suffix in suffixes_to_remove:
                    if new_col.endswith(suffix):
                        new_col = new_col.replace(suffix, '')
                        break
                
                if '(ä¸‡å…ƒ)' in new_col:
                    new_col = new_col.replace('(ä¸‡å…ƒ)', '')
                if '(æœˆ)' in new_col:
                    new_col = new_col.replace('(æœˆ)', '')
                
                new_column_mapping[old_col] = new_col
        
        used_names = set()
        final_mapping = {}
        for old_col, new_col in new_column_mapping.items():
            if new_col in used_names:
                counter = 1
                while f"{new_col}_{counter}" in used_names:
                    counter += 1
                new_col = f"{new_col}_{counter}"
            used_names.add(new_col)
            final_mapping[old_col] = new_col
        
        self.merged_data = self.merged_data.rename(columns=final_mapping)
        
        self._log_step("ä¸­æ–‡åˆ—åç®€åŒ–å®Œæˆ", f"ç®€åŒ–äº† {len(final_mapping)} ä¸ªåˆ—å")
        return self.merged_data
    
    def validate_data_quality(self):
        self._log_step("=== æ•°æ®è´¨é‡éªŒè¯ ===")
        
        quality_report = {
            'basic_info': {
                'total_rows': len(self.merged_data),
                'total_columns': len(self.merged_data.columns),
                'missing_values': self.merged_data.isnull().sum().sum(),
                'duplicate_rows': self.merged_data.duplicated().sum(),
                'infinite_values': np.isinf(self.merged_data.select_dtypes(include=[np.number])).sum().sum()
            },
            'target_distribution': {}
        }
        
        target_col = 'æ˜¯å¦æœ‰é€¾æœŸ'
        if target_col in self.merged_data.columns:
            quality_report['target_distribution'] = self.merged_data[target_col].value_counts().to_dict()
        
        self._log_step("æ•°æ®è´¨é‡éªŒè¯å®Œæˆ", 
                      f"ä¼ä¸š: {quality_report['basic_info']['total_rows']}, ç‰¹å¾: {quality_report['basic_info']['total_columns']-1}")
        
        return quality_report
    
    def save_processed_data(self, filename='../è¾“å‡ºç»“æœ/processed_data_final.xlsx'):
        self._log_step("=== ä¿å­˜å¤„ç†åçš„æ•°æ® ===")
        
        output_path = os.path.join(self.output_dir, filename)
        
        try:
            self.merged_data.to_excel(output_path, index=False)
            self.final_dataset = self.merged_data.copy()
            
            self._log_step("æ•°æ®ä¿å­˜å®Œæˆ", 
                          f"æ–‡ä»¶: {output_path}, å½¢çŠ¶: {self.merged_data.shape}")
            
            return output_path
            
        except Exception as e:
            self._log_step("æ•°æ®ä¿å­˜å¤±è´¥", str(e))
            raise e
    
    def generate_documentation(self):
        self._log_step("=== ç”Ÿæˆå¤„ç†æ–‡æ¡£ ===")
        
        quality_report = self.validate_data_quality()
        doc_content = self._create_comprehensive_documentation(quality_report)
        
        doc_filename = os.path.join(self.output_dir, '../æ–‡æ¡£/æ•°æ®å¤„ç†è¯´æ˜æ–‡æ¡£.md')
        with open(doc_filename, 'w', encoding='utf-8') as f:
            f.write(doc_content)
        
        log_filename = os.path.join(self.output_dir, 'processing_log.json')
        with open(log_filename, 'w', encoding='utf-8') as f:
            json.dump(self.processing_log, f, ensure_ascii=False, indent=2)
        
        if self.feature_importance is not None:
            importance_filename = os.path.join(self.output_dir, '../è¾“å‡ºç»“æœ/feature_importance.xlsx')
            self.feature_importance.to_excel(importance_filename, index=False)
        
        self._log_step("æ–‡æ¡£ç”Ÿæˆå®Œæˆ", f"ä¸»æ–‡æ¡£: {doc_filename}")
        return doc_filename
    
    def _create_comprehensive_documentation(self, quality_report):
        target_col = 'æ˜¯å¦æœ‰é€¾æœŸ'
        target_info = "æœªæ‰¾åˆ°ç›®æ ‡å˜é‡"
        if target_col in self.merged_data.columns:
            target_info = f"ç›®æ ‡å˜é‡ '{target_col}' æœ‰ {self.merged_data[target_col].nunique()} ä¸ªå”¯ä¸€å€¼"
        
        encoded_examples = []
        for feature in self.encoded_features[:5]:
            if feature in self.merged_data.columns:
                encoded_examples.append(f"- {feature}: {self.merged_data[feature].nunique()} ä¸ªç±»åˆ«")
        
        top_features = []
        if self.feature_importance is not None:
            for _, row in self.feature_importance.head(10).iterrows():
                top_features.append(f"| {row['feature']} | {row['importance']:.4f} |")
        
        doc_content = f"""# ä¸­å°ä¼ä¸šä¿¡è´·é£é™©è¯„ä¼° - æ•°æ®å¤„ç†è¯´æ˜æ–‡æ¡£


**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  


## ğŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯

### åŸå§‹æ•°æ®æº
æœ¬é¡¹ç›®æ•´åˆäº†ä»¥ä¸‹10ä¸ªæ•°æ®æºï¼š

| åºå· | æ•°æ®æº | æ–‡ä»¶å | ä¸»è¦å†…å®¹ |
|------|--------|--------|----------|
| 1 | ä¼ä¸šåŸºæœ¬ä¿¡æ¯ | 01_ä¼ä¸šåŸºæœ¬ä¿¡æ¯.xlsx | æ³¨å†Œèµ„æœ¬ã€æˆç«‹å¹´é™ã€å‘˜å·¥æ•°ç­‰ |
| 2 | ä¿¡è´·ç”³è¯·æ•°æ® | 02_ä¿¡è´·ç”³è¯·æ•°æ®.xlsx | ç”³è¯·é‡‘é¢ã€æœŸé™ã€æ‹…ä¿æ–¹å¼ç­‰ |
| 3 | è´¢åŠ¡æŠ¥è¡¨æ•°æ® | 03_è´¢åŠ¡æŠ¥è¡¨æ•°æ®.xlsx | èµ„äº§è´Ÿå€ºã€æ”¶å…¥åˆ©æ¶¦ã€è´¢åŠ¡æ¯”ç‡ç­‰ |
| 4 | ç¨åŠ¡è®°å½• | 04_ç¨åŠ¡è®°å½•.xlsx | çº³ç¨ä¿¡ç”¨ç­‰çº§ã€ç¨åŠ¡åˆè§„æƒ…å†µç­‰ |
| 5 | é“¶è¡Œæµæ°´ | 05_é“¶è¡Œæµæ°´.xlsx | æœˆå‡æ”¶æ”¯ã€è´¦æˆ·ä½™é¢ã€æµæ°´ç¨³å®šæ€§ç­‰ |
| 6 | ä¼ä¸šä¸»ä¿¡ç”¨æ•°æ® | 06_ä¼ä¸šä¸»ä¿¡ç”¨æ•°æ®.xlsx | ä¸ªäººæ”¶å…¥èµ„äº§ã€æ•™è‚²èƒŒæ™¯ã€ä¿¡ç”¨è®°å½•ç­‰ |
| 7 | è¡Œä¸šç»è¥çŠ¶å†µ | 07_è¡Œä¸šç»è¥çŠ¶å†µ.xlsx | è¡Œä¸šå¢é•¿ç‡ã€ç«äº‰ç¨‹åº¦ã€é£é™©ç­‰çº§ç­‰ |
| 8 | ä¼ä¸šå¾ä¿¡æŠ¥å‘Š | 08_ä¼ä¸šå¾ä¿¡æŠ¥å‘Š.xlsx | ä¿¡ç”¨è¯„åˆ†ã€é€¾æœŸè®°å½•ã€ä¸è‰¯è®°å½•ç­‰ |
| 9 | ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ | 09_ä¸Šä¸‹æ¸¸åˆä½œæƒ…å†µ.xlsx | ä¾›åº”é“¾ç¨³å®šæ€§ã€å®¢æˆ·ä¾›åº”å•†æ•°é‡ç­‰ |
| 10 | ä¼ä¸šç»è¥é£é™©è¯„ä¼° | 10_ä¼ä¸šç»è¥é£é™©è¯„ä¼°.xlsx | å¸‚åœºé£é™©ã€ç»è¥é£é™©ã€è´¢åŠ¡é£é™©ç­‰ |


| **ä¼ä¸šæ•°é‡** | {quality_report['basic_info']['total_rows']} |
| **ç‰¹å¾æ•°é‡** | {quality_report['basic_info']['total_columns']-1} |
| **æ€»åˆ—æ•°** | {quality_report['basic_info']['total_columns']} |
| **ç›®æ ‡å˜é‡** | è´·æ¬¾é€¾æœŸæ¬¡æ•° |
| **æ•°æ®æ–‡ä»¶å¤§å°** | çº¦ 0.11 MB |


| **ç¼ºå¤±å€¼** | {quality_report['basic_info']['missing_values']} |
| **é‡å¤è¡Œ** | {quality_report['basic_info']['duplicate_rows']} |
| **æ— ç©·å€¼** | {quality_report['basic_info']['infinite_values']} |

{target_info}

{chr(10).join(encoded_examples)}


{chr(10).join(top_features)}


**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        return doc_content
    
    def run_complete_pipeline(self, save_filename='../è¾“å‡ºç»“æœ/processed_data_final.xlsx'):
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®é¢„å¤„ç†ç®¡é“")
        print("="*80)
        
        try:
            self.load_and_aggregate_data()
            self.clean_features()
            self.handle_missing_values()
            self.encode_categorical_features()
            self.feature_selection()
            self.optimize_standardization()
            self.simplify_chinese_names()
            quality_report = self.validate_data_quality()
            data_file = self.save_processed_data(save_filename)
            doc_file = self.generate_documentation()
            
            print("\n" + "="*80)
            print("ğŸ‰ å®Œæ•´æ•°æ®é¢„å¤„ç†ç®¡é“æ‰§è¡Œå®Œæˆï¼")
            print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é›†: {data_file}")
            print(f"ğŸ“‹ è¯´æ˜æ–‡æ¡£: {doc_file}")
            print(f"ğŸ“ˆ æ•°æ®è§„æ¨¡: {self.merged_data.shape[0]} ä¼ä¸š Ã— {self.merged_data.shape[1]-1} ç‰¹å¾")
            
            return data_file, doc_file
            
        except Exception as e:
            self._log_step("å¤„ç†è¿‡ç¨‹å‡ºç°é”™è¯¯", str(e))
            raise e

def main():
    print("ä¸­å°ä¼ä¸šä¿¡è´·é£é™©è¯„ä¼° - æ•°æ®é¢„å¤„ç†ç³»ç»Ÿ")
    print("ç‰ˆæœ¬: 3.0 (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ)")
    print("="*80)
    
    processor = ComprehensiveDataProcessor()
    data_file, doc_file = processor.run_complete_pipeline()
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {data_file}")
    print(f"ğŸ“‹ æ–‡æ¡£æ–‡ä»¶: {doc_file}")
    
    return data_file, doc_file

if __name__ == "__main__":
    main()