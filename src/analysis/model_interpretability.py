"""
æ¨¡å‹è§£é‡Šæ€§å·¥å…·
é›†æˆSHAPã€LIMEç­‰å·¥å…·å®ç°æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ
"""

import os
import sys
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio

from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
import json

# æ¨¡å‹è§£é‡Šæ€§åº“
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("âš ï¸  SHAPæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    import lime
    import lime.lime_tabular
    LIME_AVAILABLE = True
except ImportError:
    LIME_AVAILABLE = False
    print("âš ï¸  LIMEæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")

# æœºå™¨å­¦ä¹ åº“
from sklearn.inspection import permutation_importance
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

warnings.filterwarnings('ignore')

class ModelInterpretabilityAnalyzer:
    """æ¨¡å‹è§£é‡Šæ€§åˆ†æå™¨"""
    
    def __init__(self, output_dir: str = "interpretability_analysis"):
        """
        åˆå§‹åŒ–æ¨¡å‹è§£é‡Šæ€§åˆ†æå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.charts_dir = os.path.join(output_dir, "charts")
        self.reports_dir = os.path.join(output_dir, "reports")
        self.data_dir = os.path.join(output_dir, "data")
        
        for dir_path in [self.charts_dir, self.reports_dir, self.data_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # åˆå§‹åŒ–è§£é‡Šå™¨
        self.shap_explainers = {}
        self.lime_explainers = {}
        
        # é…ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
        plt.rcParams['axes.unicode_minus'] = False
    
    def comprehensive_interpretability_analysis(self, 
                                              models: Dict[str, Any],
                                              X_train: np.ndarray,
                                              X_test: np.ndarray,
                                              y_train: np.ndarray,
                                              y_test: np.ndarray,
                                              feature_names: List[str] = None) -> Dict:
        """
        ç»¼åˆå¯è§£é‡Šæ€§åˆ†æ
        
        Args:
            models: æ¨¡å‹å­—å…¸
            X_train: è®­ç»ƒç‰¹å¾
            X_test: æµ‹è¯•ç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            y_test: æµ‹è¯•æ ‡ç­¾
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            
        Returns:
            è§£é‡Šæ€§åˆ†æç»“æœ
        """
        
        print("ğŸ” å¼€å§‹ç»¼åˆå¯è§£é‡Šæ€§åˆ†æ...")
        
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]
        
        results = {
            'global_explanations': {},
            'local_explanations': {},
            'feature_importance': {},
            'interaction_analysis': {},
            'model_behavior': {},
            'charts': {},
            'summary': {}
        }
        
        for model_name, model in models.items():
            print(f"  ğŸ“Š åˆ†ææ¨¡å‹: {model_name}")
            
            try:
                # å…¨å±€è§£é‡Šæ€§åˆ†æ
                global_exp = self._analyze_global_interpretability(
                    model, model_name, X_train, X_test, y_train, feature_names
                )
                results['global_explanations'][model_name] = global_exp
                
                # å±€éƒ¨è§£é‡Šæ€§åˆ†æ
                local_exp = self._analyze_local_interpretability(
                    model, model_name, X_train, X_test, y_train, feature_names
                )
                results['local_explanations'][model_name] = local_exp
                
                # ç‰¹å¾é‡è¦æ€§åˆ†æ
                feature_imp = self._analyze_feature_importance(
                    model, model_name, X_train, X_test, y_train, y_test, feature_names
                )
                results['feature_importance'][model_name] = feature_imp
                
                # ç‰¹å¾äº¤äº’åˆ†æ
                interaction_analysis = self._analyze_feature_interactions(
                    model, model_name, X_train, feature_names
                )
                results['interaction_analysis'][model_name] = interaction_analysis
                
                # æ¨¡å‹è¡Œä¸ºåˆ†æ
                behavior_analysis = self._analyze_model_behavior(
                    model, model_name, X_train, X_test, y_train, feature_names
                )
                results['model_behavior'][model_name] = behavior_analysis
                
            except Exception as e:
                print(f"    âŒ {model_name} åˆ†æå¤±è´¥: {str(e)}")
                results['global_explanations'][model_name] = {'error': str(e)}
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        results['comparison'] = self._compare_model_interpretability(results)
        
        # ç”Ÿæˆå›¾è¡¨
        results['charts'] = self._generate_interpretability_charts(results, feature_names)
        
        # ç”Ÿæˆæ‘˜è¦
        results['summary'] = self._generate_interpretability_summary(results)
        
        print("âœ… ç»¼åˆå¯è§£é‡Šæ€§åˆ†æå®Œæˆ")
        return results
    
    def _analyze_global_interpretability(self, 
                                       model: Any,
                                       model_name: str,
                                       X_train: np.ndarray,
                                       X_test: np.ndarray,
                                       y_train: np.ndarray,
                                       feature_names: List[str]) -> Dict:
        """åˆ†æå…¨å±€å¯è§£é‡Šæ€§"""
        
        global_analysis = {
            'shap_analysis': None,
            'permutation_importance': None,
            'model_specific_importance': None
        }
        
        try:
            # SHAPå…¨å±€åˆ†æ
            if SHAP_AVAILABLE:
                global_analysis['shap_analysis'] = self._shap_global_analysis(
                    model, model_name, X_train, X_test, feature_names
                )
            
            # æ’åˆ—é‡è¦æ€§
            global_analysis['permutation_importance'] = self._permutation_importance_analysis(
                model, X_test, y_train, feature_names
            )
            
            # æ¨¡å‹ç‰¹å®šé‡è¦æ€§
            global_analysis['model_specific_importance'] = self._model_specific_importance(
                model, feature_names
            )
            
        except Exception as e:
            global_analysis['error'] = str(e)
        
        return global_analysis
    
    def _shap_global_analysis(self, 
                            model: Any,
                            model_name: str,
                            X_train: np.ndarray,
                            X_test: np.ndarray,
                            feature_names: List[str]) -> Dict:
        """SHAPå…¨å±€åˆ†æ"""
        
        if not SHAP_AVAILABLE:
            return {'error': 'SHAP not available'}
        
        try:
            # é€‰æ‹©åˆé€‚çš„è§£é‡Šå™¨
            if hasattr(model, 'predict_proba'):
                # å¯¹äºæ¦‚ç‡é¢„æµ‹æ¨¡å‹
                if 'tree' in model_name.lower() or 'forest' in model_name.lower():
                    explainer = shap.TreeExplainer(model)
                else:
                    explainer = shap.Explainer(model, X_train[:100])  # ä½¿ç”¨æ ·æœ¬ä½œä¸ºèƒŒæ™¯
            else:
                explainer = shap.Explainer(model, X_train[:100])
            
            # è®¡ç®—SHAPå€¼
            shap_values = explainer(X_test[:200])  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜é€Ÿåº¦
            
            # å…¨å±€ç‰¹å¾é‡è¦æ€§
            if hasattr(shap_values, 'values'):
                if len(shap_values.values.shape) == 3:  # å¤šç±»åˆ†ç±»
                    global_importance = np.mean(np.abs(shap_values.values[:, :, 1]), axis=0)
                else:
                    global_importance = np.mean(np.abs(shap_values.values), axis=0)
            else:
                global_importance = np.mean(np.abs(shap_values), axis=0)
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': global_importance
            }).sort_values('importance', ascending=False)
            
            # ä¿å­˜SHAPå›¾è¡¨
            self._save_shap_plots(shap_values, feature_names, model_name)
            
            return {
                'explainer_type': type(explainer).__name__,
                'feature_importance': importance_df.to_dict('records'),
                'shap_values_shape': shap_values.values.shape if hasattr(shap_values, 'values') else np.array(shap_values).shape,
                'top_features': importance_df.head(10).to_dict('records')
            }
            
        except Exception as e:
            return {'error': f'SHAP analysis failed: {str(e)}'}
    
    def _save_shap_plots(self, shap_values, feature_names: List[str], model_name: str):
        """ä¿å­˜SHAPå›¾è¡¨"""
        
        try:
            # æ‘˜è¦å›¾
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values, feature_names=feature_names, show=False)
            plt.title(f'{model_name} - SHAPç‰¹å¾é‡è¦æ€§æ‘˜è¦')
            plt.tight_layout()
            plt.savefig(os.path.join(self.charts_dir, f'{model_name}_shap_summary.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            # æ¡å½¢å›¾
            plt.figure(figsize=(10, 6))
            shap.summary_plot(shap_values, feature_names=feature_names, plot_type="bar", show=False)
            plt.title(f'{model_name} - SHAPç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾')
            plt.tight_layout()
            plt.savefig(os.path.join(self.charts_dir, f'{model_name}_shap_bar.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"    âš ï¸  SHAPå›¾è¡¨ä¿å­˜å¤±è´¥: {str(e)}")
    
    def _permutation_importance_analysis(self, 
                                       model: Any,
                                       X_test: np.ndarray,
                                       y_test: np.ndarray,
                                       feature_names: List[str]) -> Dict:
        """æ’åˆ—é‡è¦æ€§åˆ†æ"""
        
        try:
            # è®¡ç®—æ’åˆ—é‡è¦æ€§
            perm_importance = permutation_importance(
                model, X_test, y_test, n_repeats=10, random_state=42
            )
            
            # åˆ›å»ºé‡è¦æ€§DataFrame
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance_mean': perm_importance.importances_mean,
                'importance_std': perm_importance.importances_std
            }).sort_values('importance_mean', ascending=False)
            
            return {
                'feature_importance': importance_df.to_dict('records'),
                'top_features': importance_df.head(10).to_dict('records'),
                'statistics': {
                    'mean_importance': float(np.mean(perm_importance.importances_mean)),
                    'std_importance': float(np.std(perm_importance.importances_mean)),
                    'max_importance': float(np.max(perm_importance.importances_mean)),
                    'min_importance': float(np.min(perm_importance.importances_mean))
                }
            }
            
        except Exception as e:
            return {'error': f'Permutation importance analysis failed: {str(e)}'}
    
    def _model_specific_importance(self, model: Any, feature_names: List[str]) -> Dict:
        """æ¨¡å‹ç‰¹å®šé‡è¦æ€§åˆ†æ"""
        
        try:
            importance_data = {}
            
            # çº¿æ€§æ¨¡å‹ç³»æ•°
            if hasattr(model, 'coef_'):
                coef = model.coef_
                if len(coef.shape) > 1:
                    coef = coef[0]  # å–ç¬¬ä¸€ç±»çš„ç³»æ•°
                
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'coefficient': coef,
                    'abs_coefficient': np.abs(coef)
                }).sort_values('abs_coefficient', ascending=False)
                
                importance_data['linear_coefficients'] = importance_df.to_dict('records')
            
            # æ ‘æ¨¡å‹ç‰¹å¾é‡è¦æ€§
            if hasattr(model, 'feature_importances_'):
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                importance_data['tree_importance'] = importance_df.to_dict('records')
            
            # é›†æˆæ¨¡å‹ç‰¹å¾é‡è¦æ€§
            if hasattr(model, 'estimators_'):
                if hasattr(model.estimators_[0], 'feature_importances_'):
                    # è®¡ç®—æ‰€æœ‰åŸºå­¦ä¹ å™¨çš„å¹³å‡é‡è¦æ€§
                    importances = np.array([est.feature_importances_ for est in model.estimators_])
                    mean_importance = np.mean(importances, axis=0)
                    std_importance = np.std(importances, axis=0)
                    
                    importance_df = pd.DataFrame({
                        'feature': feature_names,
                        'importance_mean': mean_importance,
                        'importance_std': std_importance
                    }).sort_values('importance_mean', ascending=False)
                    
                    importance_data['ensemble_importance'] = importance_df.to_dict('records')
            
            return importance_data
            
        except Exception as e:
            return {'error': f'Model-specific importance analysis failed: {str(e)}'}
    
    def _analyze_local_interpretability(self, 
                                      model: Any,
                                      model_name: str,
                                      X_train: np.ndarray,
                                      X_test: np.ndarray,
                                      y_train: np.ndarray,
                                      feature_names: List[str]) -> Dict:
        """åˆ†æå±€éƒ¨å¯è§£é‡Šæ€§"""
        
        local_analysis = {
            'shap_local': None,
            'lime_local': None,
            'sample_explanations': []
        }
        
        try:
            # é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬è¿›è¡Œå±€éƒ¨è§£é‡Š
            sample_indices = self._select_representative_samples(X_test, y_train, n_samples=5)
            
            # SHAPå±€éƒ¨åˆ†æ
            if SHAP_AVAILABLE:
                local_analysis['shap_local'] = self._shap_local_analysis(
                    model, model_name, X_train, X_test, sample_indices, feature_names
                )
            
            # LIMEå±€éƒ¨åˆ†æ
            if LIME_AVAILABLE:
                local_analysis['lime_local'] = self._lime_local_analysis(
                    model, model_name, X_train, X_test, sample_indices, feature_names
                )
            
            # æ ·æœ¬è§£é‡Š
            for idx in sample_indices:
                sample_explanation = self._explain_single_sample(
                    model, X_test[idx:idx+1], feature_names, idx
                )
                local_analysis['sample_explanations'].append(sample_explanation)
            
        except Exception as e:
            local_analysis['error'] = str(e)
        
        return local_analysis
    
    def _select_representative_samples(self, X_test: np.ndarray, y_train: np.ndarray, n_samples: int = 5) -> List[int]:
        """é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬"""
        
        try:
            # ç®€å•ç­–ç•¥ï¼šé€‰æ‹©ä¸åŒåŒºåŸŸçš„æ ·æœ¬
            indices = []
            
            # éšæœºé€‰æ‹©
            np.random.seed(42)
            random_indices = np.random.choice(len(X_test), size=min(n_samples, len(X_test)), replace=False)
            indices.extend(random_indices)
            
            return list(set(indices))[:n_samples]
            
        except Exception as e:
            # å¦‚æœå¤±è´¥ï¼Œè¿”å›å‰nä¸ªæ ·æœ¬
            return list(range(min(n_samples, len(X_test))))
    
    def _shap_local_analysis(self, 
                           model: Any,
                           model_name: str,
                           X_train: np.ndarray,
                           X_test: np.ndarray,
                           sample_indices: List[int],
                           feature_names: List[str]) -> Dict:
        """SHAPå±€éƒ¨åˆ†æ"""
        
        if not SHAP_AVAILABLE:
            return {'error': 'SHAP not available'}
        
        try:
            # åˆ›å»ºè§£é‡Šå™¨
            if 'tree' in model_name.lower() or 'forest' in model_name.lower():
                explainer = shap.TreeExplainer(model)
            else:
                explainer = shap.Explainer(model, X_train[:100])
            
            local_explanations = []
            
            for idx in sample_indices:
                sample = X_test[idx:idx+1]
                shap_values = explainer(sample)
                
                if hasattr(shap_values, 'values'):
                    if len(shap_values.values.shape) == 3:  # å¤šç±»åˆ†ç±»
                        values = shap_values.values[0, :, 1]  # å–æ­£ç±»çš„SHAPå€¼
                    else:
                        values = shap_values.values[0]
                else:
                    values = shap_values[0]
                
                # åˆ›å»ºè§£é‡ŠDataFrame
                explanation_df = pd.DataFrame({
                    'feature': feature_names,
                    'feature_value': sample[0],
                    'shap_value': values
                }).sort_values('shap_value', key=abs, ascending=False)
                
                local_explanations.append({
                    'sample_index': idx,
                    'explanation': explanation_df.to_dict('records'),
                    'prediction': float(model.predict_proba(sample)[0][1]) if hasattr(model, 'predict_proba') else float(model.predict(sample)[0])
                })
                
                # ä¿å­˜å•ä¸ªæ ·æœ¬çš„SHAPå›¾
                self._save_single_shap_plot(shap_values, feature_names, model_name, idx)
            
            return {
                'explanations': local_explanations,
                'summary': f'Generated local explanations for {len(sample_indices)} samples'
            }
            
        except Exception as e:
            return {'error': f'SHAP local analysis failed: {str(e)}'}
    
    def _save_single_shap_plot(self, shap_values, feature_names: List[str], model_name: str, sample_idx: int):
        """ä¿å­˜å•ä¸ªæ ·æœ¬çš„SHAPå›¾"""
        
        try:
            plt.figure(figsize=(10, 6))
            shap.waterfall_plot(shap_values[0], show=False)
            plt.title(f'{model_name} - æ ·æœ¬{sample_idx} SHAPè§£é‡Š')
            plt.tight_layout()
            plt.savefig(os.path.join(self.charts_dir, f'{model_name}_sample_{sample_idx}_shap.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"    âš ï¸  æ ·æœ¬{sample_idx} SHAPå›¾ä¿å­˜å¤±è´¥: {str(e)}")
    
    def _lime_local_analysis(self, 
                           model: Any,
                           model_name: str,
                           X_train: np.ndarray,
                           X_test: np.ndarray,
                           sample_indices: List[int],
                           feature_names: List[str]) -> Dict:
        """LIMEå±€éƒ¨åˆ†æ"""
        
        if not LIME_AVAILABLE:
            return {'error': 'LIME not available'}
        
        try:
            # åˆ›å»ºLIMEè§£é‡Šå™¨
            explainer = lime.lime_tabular.LimeTabularExplainer(
                X_train,
                feature_names=feature_names,
                class_names=['ä½é£é™©', 'é«˜é£é™©'],
                mode='classification'
            )
            
            local_explanations = []
            
            for idx in sample_indices:
                sample = X_test[idx]
                
                # ç”Ÿæˆè§£é‡Š
                explanation = explainer.explain_instance(
                    sample, 
                    model.predict_proba if hasattr(model, 'predict_proba') else model.predict,
                    num_features=len(feature_names)
                )
                
                # æå–è§£é‡Šæ•°æ®
                explanation_data = []
                for feature, weight in explanation.as_list():
                    explanation_data.append({
                        'feature': feature,
                        'weight': weight,
                        'abs_weight': abs(weight)
                    })
                
                # æŒ‰é‡è¦æ€§æ’åº
                explanation_data.sort(key=lambda x: x['abs_weight'], reverse=True)
                
                local_explanations.append({
                    'sample_index': idx,
                    'explanation': explanation_data,
                    'prediction_proba': explanation.predict_proba.tolist() if hasattr(explanation, 'predict_proba') else None
                })
                
                # ä¿å­˜LIMEå›¾
                self._save_lime_plot(explanation, model_name, idx)
            
            return {
                'explanations': local_explanations,
                'summary': f'Generated LIME explanations for {len(sample_indices)} samples'
            }
            
        except Exception as e:
            return {'error': f'LIME local analysis failed: {str(e)}'}
    
    def _save_lime_plot(self, explanation, model_name: str, sample_idx: int):
        """ä¿å­˜LIMEå›¾"""
        
        try:
            # ä¿å­˜ä¸ºHTML
            html_path = os.path.join(self.charts_dir, f'{model_name}_sample_{sample_idx}_lime.html')
            explanation.save_to_file(html_path)
            
        except Exception as e:
            print(f"    âš ï¸  æ ·æœ¬{sample_idx} LIMEå›¾ä¿å­˜å¤±è´¥: {str(e)}")
    
    def _explain_single_sample(self, 
                             model: Any,
                             sample: np.ndarray,
                             feature_names: List[str],
                             sample_idx: int) -> Dict:
        """è§£é‡Šå•ä¸ªæ ·æœ¬"""
        
        try:
            # åŸºæœ¬é¢„æµ‹ä¿¡æ¯
            prediction = model.predict(sample)[0]
            prediction_proba = model.predict_proba(sample)[0] if hasattr(model, 'predict_proba') else None
            
            # ç‰¹å¾å€¼åˆ†æ
            feature_analysis = []
            for i, (feature_name, value) in enumerate(zip(feature_names, sample[0])):
                feature_analysis.append({
                    'feature': feature_name,
                    'value': float(value),
                    'normalized_value': float((value - np.mean(sample)) / (np.std(sample) + 1e-8))
                })
            
            return {
                'sample_index': sample_idx,
                'prediction': int(prediction),
                'prediction_proba': prediction_proba.tolist() if prediction_proba is not None else None,
                'feature_values': feature_analysis,
                'risk_level': 'é«˜é£é™©' if prediction == 1 else 'ä½é£é™©'
            }
            
        except Exception as e:
            return {'error': f'Sample explanation failed: {str(e)}'}
    
    def _analyze_feature_importance(self, 
                                  model: Any,
                                  model_name: str,
                                  X_train: np.ndarray,
                                  X_test: np.ndarray,
                                  y_train: np.ndarray,
                                  y_test: np.ndarray,
                                  feature_names: List[str]) -> Dict:
        """ç‰¹å¾é‡è¦æ€§ç»¼åˆåˆ†æ"""
        
        importance_analysis = {
            'methods_comparison': {},
            'stability_analysis': {},
            'correlation_analysis': {}
        }
        
        try:
            # å¤šç§æ–¹æ³•çš„ç‰¹å¾é‡è¦æ€§
            methods = {}
            
            # 1. æ’åˆ—é‡è¦æ€§
            perm_imp = permutation_importance(model, X_test, y_test, n_repeats=5, random_state=42)
            methods['permutation'] = perm_imp.importances_mean
            
            # 2. æ¨¡å‹å†…ç½®é‡è¦æ€§
            if hasattr(model, 'feature_importances_'):
                methods['builtin'] = model.feature_importances_
            elif hasattr(model, 'coef_'):
                coef = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_
                methods['coefficients'] = np.abs(coef)
            
            # 3. SHAPé‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if SHAP_AVAILABLE:
                try:
                    if 'tree' in model_name.lower() or 'forest' in model_name.lower():
                        explainer = shap.TreeExplainer(model)
                    else:
                        explainer = shap.Explainer(model, X_train[:50])
                    
                    shap_values = explainer(X_test[:100])
                    if hasattr(shap_values, 'values'):
                        if len(shap_values.values.shape) == 3:
                            shap_importance = np.mean(np.abs(shap_values.values[:, :, 1]), axis=0)
                        else:
                            shap_importance = np.mean(np.abs(shap_values.values), axis=0)
                    else:
                        shap_importance = np.mean(np.abs(shap_values), axis=0)
                    
                    methods['shap'] = shap_importance
                except:
                    pass
            
            # åˆ›å»ºå¯¹æ¯”DataFrame
            comparison_data = {'feature': feature_names}
            for method_name, importance in methods.items():
                comparison_data[method_name] = importance
            
            comparison_df = pd.DataFrame(comparison_data)
            importance_analysis['methods_comparison'] = comparison_df.to_dict('records')
            
            # ç¨³å®šæ€§åˆ†æ
            if len(methods) > 1:
                # è®¡ç®—ä¸åŒæ–¹æ³•ä¹‹é—´çš„ç›¸å…³æ€§
                method_names = list(methods.keys())
                correlations = {}
                
                for i, method1 in enumerate(method_names):
                    for j, method2 in enumerate(method_names[i+1:], i+1):
                        corr = np.corrcoef(methods[method1], methods[method2])[0, 1]
                        correlations[f'{method1}_vs_{method2}'] = float(corr)
                
                importance_analysis['stability_analysis'] = {
                    'method_correlations': correlations,
                    'average_correlation': float(np.mean(list(correlations.values()))),
                    'stability_score': float(np.mean(list(correlations.values())))
                }
            
            # ç‰¹å¾ç›¸å…³æ€§åˆ†æ
            feature_corr = np.corrcoef(X_train.T)
            high_corr_pairs = []
            
            for i in range(len(feature_names)):
                for j in range(i+1, len(feature_names)):
                    corr = feature_corr[i, j]
                    if abs(corr) > 0.7:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                        high_corr_pairs.append({
                            'feature1': feature_names[i],
                            'feature2': feature_names[j],
                            'correlation': float(corr)
                        })
            
            importance_analysis['correlation_analysis'] = {
                'high_correlation_pairs': high_corr_pairs,
                'feature_correlation_matrix': feature_corr.tolist()
            }
            
        except Exception as e:
            importance_analysis['error'] = str(e)
        
        return importance_analysis
    
    def _analyze_feature_interactions(self, 
                                    model: Any,
                                    model_name: str,
                                    X_train: np.ndarray,
                                    feature_names: List[str]) -> Dict:
        """ç‰¹å¾äº¤äº’åˆ†æ"""
        
        interaction_analysis = {
            'shap_interactions': None,
            'statistical_interactions': None,
            'pairwise_analysis': None
        }
        
        try:
            # SHAPäº¤äº’åˆ†æ
            if SHAP_AVAILABLE and len(feature_names) <= 20:  # é™åˆ¶ç‰¹å¾æ•°é‡ä»¥æé«˜é€Ÿåº¦
                try:
                    if 'tree' in model_name.lower() or 'forest' in model_name.lower():
                        explainer = shap.TreeExplainer(model)
                        shap_interaction_values = explainer.shap_interaction_values(X_train[:100])
                        
                        # è®¡ç®—äº¤äº’å¼ºåº¦
                        interaction_strength = np.abs(shap_interaction_values).mean(axis=0)
                        
                        # æ‰¾åˆ°æœ€å¼ºçš„äº¤äº’
                        top_interactions = []
                        for i in range(len(feature_names)):
                            for j in range(i+1, len(feature_names)):
                                strength = interaction_strength[i, j]
                                top_interactions.append({
                                    'feature1': feature_names[i],
                                    'feature2': feature_names[j],
                                    'interaction_strength': float(strength)
                                })
                        
                        top_interactions.sort(key=lambda x: x['interaction_strength'], reverse=True)
                        
                        interaction_analysis['shap_interactions'] = {
                            'top_interactions': top_interactions[:10],
                            'interaction_matrix_shape': interaction_strength.shape
                        }
                except Exception as e:
                    interaction_analysis['shap_interactions'] = {'error': str(e)}
            
            # ç»Ÿè®¡äº¤äº’åˆ†æ
            interaction_analysis['statistical_interactions'] = self._statistical_interaction_analysis(
                X_train, feature_names
            )
            
        except Exception as e:
            interaction_analysis['error'] = str(e)
        
        return interaction_analysis
    
    def _statistical_interaction_analysis(self, X: np.ndarray, feature_names: List[str]) -> Dict:
        """ç»Ÿè®¡äº¤äº’åˆ†æ"""
        
        try:
            # è®¡ç®—ç‰¹å¾é—´çš„ç»Ÿè®¡å…³ç³»
            correlations = np.corrcoef(X.T)
            
            # æ‰¾åˆ°å¼ºç›¸å…³çš„ç‰¹å¾å¯¹
            strong_correlations = []
            for i in range(len(feature_names)):
                for j in range(i+1, len(feature_names)):
                    corr = correlations[i, j]
                    if abs(corr) > 0.3:  # ç›¸å…³æ€§é˜ˆå€¼
                        strong_correlations.append({
                            'feature1': feature_names[i],
                            'feature2': feature_names[j],
                            'correlation': float(corr),
                            'correlation_strength': 'strong' if abs(corr) > 0.7 else 'moderate'
                        })
            
            strong_correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)
            
            return {
                'strong_correlations': strong_correlations[:20],
                'correlation_statistics': {
                    'mean_abs_correlation': float(np.mean(np.abs(correlations[np.triu_indices_from(correlations, k=1)]))),
                    'max_correlation': float(np.max(np.abs(correlations[np.triu_indices_from(correlations, k=1)]))),
                    'num_strong_correlations': len([c for c in strong_correlations if abs(c['correlation']) > 0.7])
                }
            }
            
        except Exception as e:
            return {'error': f'Statistical interaction analysis failed: {str(e)}'}
    
    def _analyze_model_behavior(self, 
                              model: Any,
                              model_name: str,
                              X_train: np.ndarray,
                              X_test: np.ndarray,
                              y_train: np.ndarray,
                              feature_names: List[str]) -> Dict:
        """æ¨¡å‹è¡Œä¸ºåˆ†æ"""
        
        behavior_analysis = {
            'decision_boundary': None,
            'prediction_distribution': None,
            'confidence_analysis': None,
            'robustness_analysis': None
        }
        
        try:
            # é¢„æµ‹åˆ†å¸ƒåˆ†æ
            if hasattr(model, 'predict_proba'):
                y_proba = model.predict_proba(X_test)
                
                behavior_analysis['prediction_distribution'] = {
                    'class_distribution': {
                        'class_0': float(np.sum(y_proba[:, 0] > 0.5)),
                        'class_1': float(np.sum(y_proba[:, 1] > 0.5))
                    },
                    'confidence_stats': {
                        'mean_confidence': float(np.mean(np.max(y_proba, axis=1))),
                        'std_confidence': float(np.std(np.max(y_proba, axis=1))),
                        'low_confidence_samples': float(np.sum(np.max(y_proba, axis=1) < 0.6))
                    }
                }
            
            # é²æ£’æ€§åˆ†æ
            behavior_analysis['robustness_analysis'] = self._robustness_analysis(
                model, X_test, feature_names
            )
            
        except Exception as e:
            behavior_analysis['error'] = str(e)
        
        return behavior_analysis
    
    def _robustness_analysis(self, model: Any, X_test: np.ndarray, feature_names: List[str]) -> Dict:
        """é²æ£’æ€§åˆ†æ"""
        
        try:
            # æ·»åŠ å™ªå£°æµ‹è¯•
            noise_levels = [0.01, 0.05, 0.1]
            robustness_results = []
            
            original_predictions = model.predict(X_test)
            
            for noise_level in noise_levels:
                # æ·»åŠ é«˜æ–¯å™ªå£°
                noise = np.random.normal(0, noise_level, X_test.shape)
                X_noisy = X_test + noise
                
                noisy_predictions = model.predict(X_noisy)
                
                # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§
                consistency = np.mean(original_predictions == noisy_predictions)
                
                robustness_results.append({
                    'noise_level': noise_level,
                    'prediction_consistency': float(consistency),
                    'changed_predictions': int(np.sum(original_predictions != noisy_predictions))
                })
            
            return {
                'noise_robustness': robustness_results,
                'overall_robustness_score': float(np.mean([r['prediction_consistency'] for r in robustness_results]))
            }
            
        except Exception as e:
            return {'error': f'Robustness analysis failed: {str(e)}'}
    
    def _compare_model_interpretability(self, results: Dict) -> Dict:
        """å¯¹æ¯”æ¨¡å‹å¯è§£é‡Šæ€§"""
        
        comparison = {
            'interpretability_scores': {},
            'method_availability': {},
            'complexity_analysis': {}
        }
        
        try:
            for model_name in results['global_explanations'].keys():
                if 'error' not in results['global_explanations'][model_name]:
                    # è®¡ç®—å¯è§£é‡Šæ€§è¯„åˆ†
                    score = 0
                    available_methods = 0
                    
                    # SHAPå¯ç”¨æ€§
                    if (results['global_explanations'][model_name].get('shap_analysis') and 
                        'error' not in results['global_explanations'][model_name]['shap_analysis']):
                        score += 30
                        available_methods += 1
                    
                    # æ’åˆ—é‡è¦æ€§å¯ç”¨æ€§
                    if (results['global_explanations'][model_name].get('permutation_importance') and
                        'error' not in results['global_explanations'][model_name]['permutation_importance']):
                        score += 20
                        available_methods += 1
                    
                    # æ¨¡å‹ç‰¹å®šé‡è¦æ€§
                    if (results['global_explanations'][model_name].get('model_specific_importance') and
                        'error' not in results['global_explanations'][model_name]['model_specific_importance']):
                        score += 25
                        available_methods += 1
                    
                    # å±€éƒ¨è§£é‡Šå¯ç”¨æ€§
                    if (results['local_explanations'][model_name] and
                        'error' not in results['local_explanations'][model_name]):
                        score += 25
                        available_methods += 1
                    
                    comparison['interpretability_scores'][model_name] = {
                        'total_score': score,
                        'available_methods': available_methods,
                        'interpretability_level': self._get_interpretability_level(score)
                    }
            
        except Exception as e:
            comparison['error'] = str(e)
        
        return comparison
    
    def _get_interpretability_level(self, score: int) -> str:
        """è·å–å¯è§£é‡Šæ€§ç­‰çº§"""
        
        if score >= 80:
            return 'é«˜'
        elif score >= 60:
            return 'ä¸­'
        elif score >= 40:
            return 'ä½'
        else:
            return 'å¾ˆä½'
    
    def _generate_interpretability_charts(self, results: Dict, feature_names: List[str]) -> Dict:
        """ç”Ÿæˆå¯è§£é‡Šæ€§å›¾è¡¨"""
        
        charts = {}
        
        try:
            # ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾
            charts['feature_importance_comparison'] = self._create_feature_importance_comparison_chart(
                results, feature_names
            )
            
            # å¯è§£é‡Šæ€§è¯„åˆ†é›·è¾¾å›¾
            charts['interpretability_radar'] = self._create_interpretability_radar_chart(results)
            
            # ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
            charts['feature_correlation_heatmap'] = self._create_feature_correlation_heatmap(results)
            
        except Exception as e:
            charts['error'] = str(e)
        
        return charts
    
    def _create_feature_importance_comparison_chart(self, results: Dict, feature_names: List[str]) -> str:
        """åˆ›å»ºç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾è¡¨"""
        
        try:
            fig = make_subplots(
                rows=len(results['feature_importance']), cols=1,
                subplot_titles=[f'{model_name} ç‰¹å¾é‡è¦æ€§' for model_name in results['feature_importance'].keys()],
                vertical_spacing=0.1
            )
            
            colors = ['blue', 'red', 'green', 'orange', 'purple']
            
            for i, (model_name, importance_data) in enumerate(results['feature_importance'].items()):
                if 'error' not in importance_data and 'methods_comparison' in importance_data:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„é‡è¦æ€§æ–¹æ³•
                    comparison_data = importance_data['methods_comparison']
                    if comparison_data:
                        df = pd.DataFrame(comparison_data)
                        
                        # é€‰æ‹©ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—ä½œä¸ºé‡è¦æ€§
                        importance_col = None
                        for col in df.columns:
                            if col != 'feature' and df[col].dtype in ['float64', 'int64']:
                                importance_col = col
                                break
                        
                        if importance_col:
                            # å–å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾
                            top_features = df.nlargest(10, importance_col)
                            
                            fig.add_trace(
                                go.Bar(
                                    x=top_features['feature'],
                                    y=top_features[importance_col],
                                    name=f'{model_name}',
                                    marker_color=colors[i % len(colors)]
                                ),
                                row=i+1, col=1
                            )
            
            fig.update_layout(
                title='æ¨¡å‹ç‰¹å¾é‡è¦æ€§å¯¹æ¯”',
                height=300 * len(results['feature_importance']),
                showlegend=False
            )
            
            chart_path = os.path.join(self.charts_dir, 'feature_importance_comparison.html')
            fig.write_html(chart_path)
            
            return chart_path
            
        except Exception as e:
            print(f"ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾åˆ›å»ºå¤±è´¥: {str(e)}")
            return None
    
    def _create_interpretability_radar_chart(self, results: Dict) -> str:
        """åˆ›å»ºå¯è§£é‡Šæ€§é›·è¾¾å›¾"""
        
        try:
            if 'comparison' not in results or 'interpretability_scores' not in results['comparison']:
                return None
            
            scores = results['comparison']['interpretability_scores']
            
            fig = go.Figure()
            
            categories = ['SHAPå¯ç”¨æ€§', 'æ’åˆ—é‡è¦æ€§', 'æ¨¡å‹ç‰¹å®šé‡è¦æ€§', 'å±€éƒ¨è§£é‡Š', 'æ•´ä½“è¯„åˆ†']
            
            for model_name, score_data in scores.items():
                # æ„å»ºé›·è¾¾å›¾æ•°æ®
                values = [
                    score_data.get('total_score', 0) / 100 * 30,  # SHAP
                    score_data.get('total_score', 0) / 100 * 20,  # æ’åˆ—é‡è¦æ€§
                    score_data.get('total_score', 0) / 100 * 25,  # æ¨¡å‹ç‰¹å®š
                    score_data.get('total_score', 0) / 100 * 25,  # å±€éƒ¨è§£é‡Š
                    score_data.get('total_score', 0) / 100        # æ•´ä½“è¯„åˆ†
                ]
                
                fig.add_trace(go.Scatterpolar(
                    r=values,
                    theta=categories,
                    fill='toself',
                    name=model_name
                ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 1]
                    )),
                showlegend=True,
                title="æ¨¡å‹å¯è§£é‡Šæ€§è¯„ä¼°é›·è¾¾å›¾"
            )
            
            chart_path = os.path.join(self.charts_dir, 'interpretability_radar.html')
            fig.write_html(chart_path)
            
            return chart_path
            
        except Exception as e:
            print(f"å¯è§£é‡Šæ€§é›·è¾¾å›¾åˆ›å»ºå¤±è´¥: {str(e)}")
            return None
    
    def _create_feature_correlation_heatmap(self, results: Dict) -> str:
        """åˆ›å»ºç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        
        try:
            # ä»ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç›¸å…³æ€§åˆ†æä¸­è·å–æ•°æ®
            for model_name, importance_data in results['feature_importance'].items():
                if ('correlation_analysis' in importance_data and 
                    'feature_correlation_matrix' in importance_data['correlation_analysis']):
                    
                    corr_matrix = np.array(importance_data['correlation_analysis']['feature_correlation_matrix'])
                    
                    fig = go.Figure(data=go.Heatmap(
                        z=corr_matrix,
                        colorscale='RdBu',
                        zmid=0
                    ))
                    
                    fig.update_layout(
                        title='ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾',
                        xaxis_title='ç‰¹å¾',
                        yaxis_title='ç‰¹å¾'
                    )
                    
                    chart_path = os.path.join(self.charts_dir, 'feature_correlation_heatmap.html')
                    fig.write_html(chart_path)
                    
                    return chart_path
            
            return None
            
        except Exception as e:
            print(f"ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾åˆ›å»ºå¤±è´¥: {str(e)}")
            return None
    
    def _generate_interpretability_summary(self, results: Dict) -> Dict:
        """ç”Ÿæˆå¯è§£é‡Šæ€§æ‘˜è¦"""
        
        summary = {
            'total_models_analyzed': len(results['global_explanations']),
            'successful_analyses': 0,
            'available_methods': {
                'shap': SHAP_AVAILABLE,
                'lime': LIME_AVAILABLE,
                'permutation_importance': True,
                'model_specific': True
            },
            'key_findings': [],
            'recommendations': []
        }
        
        try:
            # ç»Ÿè®¡æˆåŠŸåˆ†æçš„æ¨¡å‹æ•°é‡
            for model_name, analysis in results['global_explanations'].items():
                if 'error' not in analysis:
                    summary['successful_analyses'] += 1
            
            # æå–å…³é”®å‘ç°
            if 'comparison' in results and 'interpretability_scores' in results['comparison']:
                scores = results['comparison']['interpretability_scores']
                
                # æ‰¾åˆ°æœ€å¯è§£é‡Šçš„æ¨¡å‹
                best_model = max(scores.keys(), key=lambda k: scores[k]['total_score'])
                summary['key_findings'].append(f"æœ€å¯è§£é‡Šçš„æ¨¡å‹: {best_model}")
                
                # å¹³å‡å¯è§£é‡Šæ€§è¯„åˆ†
                avg_score = np.mean([s['total_score'] for s in scores.values()])
                summary['key_findings'].append(f"å¹³å‡å¯è§£é‡Šæ€§è¯„åˆ†: {avg_score:.1f}")
            
            # ç”Ÿæˆå»ºè®®
            if SHAP_AVAILABLE:
                summary['recommendations'].append("å»ºè®®ä½¿ç”¨SHAPè¿›è¡Œæ·±å…¥çš„ç‰¹å¾é‡è¦æ€§åˆ†æ")
            else:
                summary['recommendations'].append("å»ºè®®å®‰è£…SHAPåº“ä»¥è·å¾—æ›´å¥½çš„è§£é‡Šæ€§åˆ†æ")
            
            if LIME_AVAILABLE:
                summary['recommendations'].append("å¯ä»¥ä½¿ç”¨LIMEè¿›è¡Œå±€éƒ¨è§£é‡Šåˆ†æ")
            else:
                summary['recommendations'].append("å»ºè®®å®‰è£…LIMEåº“ä»¥è¿›è¡Œå±€éƒ¨è§£é‡Šåˆ†æ")
            
            summary['recommendations'].append("å»ºè®®ç»“åˆå¤šç§è§£é‡Šæ–¹æ³•ä»¥è·å¾—æ›´å…¨é¢çš„ç†è§£")
            
        except Exception as e:
            summary['error'] = str(e)
        
        return summary
    
    def generate_interpretability_report(self, results: Dict, output_path: str = None) -> str:
        """ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š"""
        
        if output_path is None:
            output_path = os.path.join(self.reports_dir, f"interpretability_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html")
        
        try:
            # ç”ŸæˆHTMLæŠ¥å‘Š
            html_content = self._create_interpretability_html_report(results)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"âœ… å¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
    
    def _create_interpretability_html_report(self, results: Dict) -> str:
        """åˆ›å»ºå¯è§£é‡Šæ€§HTMLæŠ¥å‘Š"""
        
        html_template = """
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>æ¨¡å‹å¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š</title>
            <style>
                body { font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }
                .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                h1, h2, h3 { color: #2c3e50; }
                .summary-box { background: #ecf0f1; padding: 20px; border-radius: 8px; margin: 20px 0; }
                .model-section { border: 1px solid #ddd; margin: 20px 0; padding: 20px; border-radius: 8px; }
                .metric-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
                .metric-card { background: #f8f9fa; padding: 15px; border-radius: 6px; text-align: center; }
                .metric-value { font-size: 24px; font-weight: bold; color: #3498db; }
                .metric-label { color: #7f8c8d; margin-top: 5px; }
                table { width: 100%; border-collapse: collapse; margin: 20px 0; }
                th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
                th { background-color: #f2f2f2; font-weight: bold; }
                .chart-container { margin: 30px 0; text-align: center; }
                .recommendation { background: #d5f4e6; border-left: 4px solid #27ae60; padding: 15px; margin: 10px 0; }
                .warning { background: #fdeaa7; border-left: 4px solid #f39c12; padding: 15px; margin: 10px 0; }
                .error { background: #fadbd8; border-left: 4px solid #e74c3c; padding: 15px; margin: 10px 0; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ” æ¨¡å‹å¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š</h1>
                
                <div class="summary-box">
                    <h2>ğŸ“Š åˆ†ææ‘˜è¦</h2>
                    <div class="metric-grid">
                        <div class="metric-card">
                            <div class="metric-value">{total_models}</div>
                            <div class="metric-label">åˆ†ææ¨¡å‹æ•°é‡</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{successful_analyses}</div>
                            <div class="metric-label">æˆåŠŸåˆ†ææ•°é‡</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{shap_available}</div>
                            <div class="metric-label">SHAPå¯ç”¨</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{lime_available}</div>
                            <div class="metric-label">LIMEå¯ç”¨</div>
                        </div>
                    </div>
                </div>
                
                {model_sections}
                
                <div class="summary-box">
                    <h2>ğŸ¯ å…³é”®å‘ç°</h2>
                    {key_findings}
                </div>
                
                <div class="summary-box">
                    <h2>ğŸ’¡ å»ºè®®</h2>
                    {recommendations}
                </div>
                
                <div class="summary-box">
                    <h2>ğŸ“ˆ å›¾è¡¨åˆ†æ</h2>
                    <p>è¯¦ç»†çš„å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°chartsç›®å½•ä¸­ï¼ŒåŒ…æ‹¬ï¼š</p>
                    <ul>
                        <li>ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾</li>
                        <li>å¯è§£é‡Šæ€§è¯„ä¼°é›·è¾¾å›¾</li>
                        <li>ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾</li>
                        <li>SHAPåˆ†æå›¾è¡¨</li>
                        <li>LIMEè§£é‡Šå›¾è¡¨</li>
                    </ul>
                </div>
                
                <div style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #ddd; color: #7f8c8d; text-align: center;">
                    <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # å¡«å……æ¨¡æ¿æ•°æ®
        summary = results.get('summary', {})
        
        # ç”Ÿæˆæ¨¡å‹åˆ†æéƒ¨åˆ†
        model_sections = ""
        for model_name, global_analysis in results.get('global_explanations', {}).items():
            if 'error' not in global_analysis:
                model_sections += f"""
                <div class="model-section">
                    <h3>ğŸ¤– {model_name} åˆ†æç»“æœ</h3>
                    
                    <h4>å…¨å±€è§£é‡Šæ€§</h4>
                    {self._format_global_analysis(global_analysis)}
                    
                    <h4>å±€éƒ¨è§£é‡Šæ€§</h4>
                    {self._format_local_analysis(results.get('local_explanations', {}).get(model_name, {}))}
                    
                    <h4>ç‰¹å¾é‡è¦æ€§</h4>
                    {self._format_feature_importance(results.get('feature_importance', {}).get(model_name, {}))}
                </div>
                """
        
        # ç”Ÿæˆå…³é”®å‘ç°
        key_findings_html = ""
        for finding in summary.get('key_findings', []):
            key_findings_html += f"<p>â€¢ {finding}</p>"
        
        # ç”Ÿæˆå»ºè®®
        recommendations_html = ""
        for rec in summary.get('recommendations', []):
            recommendations_html += f'<div class="recommendation">{rec}</div>'
        
        # å¡«å……æ¨¡æ¿
        html_content = html_template.format(
            total_models=summary.get('total_models_analyzed', 0),
            successful_analyses=summary.get('successful_analyses', 0),
            shap_available="âœ…" if summary.get('available_methods', {}).get('shap', False) else "âŒ",
            lime_available="âœ…" if summary.get('available_methods', {}).get('lime', False) else "âŒ",
            model_sections=model_sections,
            key_findings=key_findings_html,
            recommendations=recommendations_html,
            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
        
        return html_content
    
    def _format_global_analysis(self, analysis: Dict) -> str:
        """æ ¼å¼åŒ–å…¨å±€åˆ†æç»“æœ"""
        
        html = ""
        
        if 'shap_analysis' in analysis and analysis['shap_analysis'] and 'error' not in analysis['shap_analysis']:
            shap_data = analysis['shap_analysis']
            html += f"""
            <p><strong>SHAPåˆ†æ:</strong> ä½¿ç”¨{shap_data.get('explainer_type', 'Unknown')}è§£é‡Šå™¨</p>
            <p>å‰5ä¸ªé‡è¦ç‰¹å¾:</p>
            <ul>
            """
            for feature in shap_data.get('top_features', [])[:5]:
                html += f"<li>{feature.get('feature', 'Unknown')}: {feature.get('importance', 0):.4f}</li>"
            html += "</ul>"
        
        if 'permutation_importance' in analysis and analysis['permutation_importance'] and 'error' not in analysis['permutation_importance']:
            perm_data = analysis['permutation_importance']
            html += f"""
            <p><strong>æ’åˆ—é‡è¦æ€§:</strong></p>
            <p>å¹³å‡é‡è¦æ€§: {perm_data.get('statistics', {}).get('mean_importance', 0):.4f}</p>
            """
        
        return html if html else "<p>æ— å¯ç”¨çš„å…¨å±€è§£é‡Šæ€§åˆ†æç»“æœ</p>"
    
    def _format_local_analysis(self, analysis: Dict) -> str:
        """æ ¼å¼åŒ–å±€éƒ¨åˆ†æç»“æœ"""
        
        if 'error' in analysis or not analysis:
            return "<p>æ— å¯ç”¨çš„å±€éƒ¨è§£é‡Šæ€§åˆ†æç»“æœ</p>"
        
        html = ""
        
        if 'shap_local' in analysis and analysis['shap_local'] and 'error' not in analysis['shap_local']:
            shap_local = analysis['shap_local']
            html += f"<p><strong>SHAPå±€éƒ¨åˆ†æ:</strong> {shap_local.get('summary', '')}</p>"
        
        if 'lime_local' in analysis and analysis['lime_local'] and 'error' not in analysis['lime_local']:
            lime_local = analysis['lime_local']
            html += f"<p><strong>LIMEå±€éƒ¨åˆ†æ:</strong> {lime_local.get('summary', '')}</p>"
        
        return html if html else "<p>æ— å¯ç”¨çš„å±€éƒ¨è§£é‡Šæ€§åˆ†æç»“æœ</p>"
    
    def _format_feature_importance(self, analysis: Dict) -> str:
        """æ ¼å¼åŒ–ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ"""
        
        if 'error' in analysis or not analysis:
            return "<p>æ— å¯ç”¨çš„ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ</p>"
        
        html = ""
        
        if 'methods_comparison' in analysis:
            html += "<p><strong>å¤šæ–¹æ³•ç‰¹å¾é‡è¦æ€§å¯¹æ¯”:</strong> å·²å®Œæˆ</p>"
        
        if 'stability_analysis' in analysis and 'stability_score' in analysis['stability_analysis']:
            stability_score = analysis['stability_analysis']['stability_score']
            html += f"<p><strong>ç¨³å®šæ€§è¯„åˆ†:</strong> {stability_score:.3f}</p>"
        
        return html if html else "<p>æ— å¯ç”¨çš„ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ</p>"


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¯è§£é‡Šæ€§åˆ†æå™¨
    analyzer = ModelInterpretabilityAnalyzer("interpretability_output")
    
    print("ğŸ” æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æå™¨å·²åˆå§‹åŒ–")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {analyzer.output_dir}")
    print(f"ğŸ› ï¸  SHAPå¯ç”¨: {SHAP_AVAILABLE}")
    print(f"ğŸ› ï¸  LIMEå¯ç”¨: {LIME_AVAILABLE}")