"""
æ€§èƒ½ç›‘æ§å’Œç¨³å®šæ€§æµ‹è¯•ç³»ç»Ÿ
å®ç°æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹çš„æ€§èƒ½ç›‘æ§ï¼Œä»¥åŠä¸åŒæ•°æ®åˆ†å¸ƒä¸‹çš„ç¨³å®šæ€§è¯„ä¼°
"""

import os
import sys
import time
import psutil
import threading
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio

from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from datetime import datetime, timedelta
from collections import defaultdict
import json
import pickle
from contextlib import contextmanager

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.base import clone

# ç»Ÿè®¡æµ‹è¯•
from scipy import stats
from scipy.stats import ks_2samp, chi2_contingency

warnings.filterwarnings('ignore')

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, output_dir: str = "performance_monitoring"):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.logs_dir = os.path.join(output_dir, "logs")
        self.charts_dir = os.path.join(output_dir, "charts")
        self.reports_dir = os.path.join(output_dir, "reports")
        
        for dir_path in [self.logs_dir, self.charts_dir, self.reports_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # ç›‘æ§æ•°æ®å­˜å‚¨
        self.performance_logs = []
        self.stability_logs = []
        self.resource_logs = []
        
        # ç›‘æ§çŠ¶æ€
        self.monitoring_active = False
        self.monitor_thread = None
        
        # é…ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
        plt.rcParams['axes.unicode_minus'] = False
    
    @contextmanager
    def monitor_performance(self, operation_name: str = "operation"):
        """
        æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            operation_name: æ“ä½œåç§°
        """
        
        # å¼€å§‹ç›‘æ§
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        start_cpu = psutil.cpu_percent()
        
        process = psutil.Process()
        start_process_memory = process.memory_info().rss
        
        try:
            yield
        finally:
            # ç»“æŸç›‘æ§
            end_time = time.time()
            end_memory = psutil.virtual_memory().used
            end_cpu = psutil.cpu_percent()
            end_process_memory = process.memory_info().rss
            
            # è®°å½•æ€§èƒ½æ•°æ®
            performance_data = {
                'operation': operation_name,
                'timestamp': datetime.now().isoformat(),
                'duration': end_time - start_time,
                'memory_usage': {
                    'start_mb': start_memory / 1024 / 1024,
                    'end_mb': end_memory / 1024 / 1024,
                    'delta_mb': (end_memory - start_memory) / 1024 / 1024
                },
                'process_memory': {
                    'start_mb': start_process_memory / 1024 / 1024,
                    'end_mb': end_process_memory / 1024 / 1024,
                    'delta_mb': (end_process_memory - start_process_memory) / 1024 / 1024
                },
                'cpu_usage': {
                    'start_percent': start_cpu,
                    'end_percent': end_cpu
                }
            }
            
            self.performance_logs.append(performance_data)
            
            # ä¿å­˜æ—¥å¿—
            self._save_performance_log(performance_data)
    
    def _save_performance_log(self, performance_data: Dict):
        """ä¿å­˜æ€§èƒ½æ—¥å¿—"""
        
        log_file = os.path.join(self.logs_dir, f"performance_{datetime.now().strftime('%Y%m%d')}.json")
        
        # è¯»å–ç°æœ‰æ—¥å¿—
        logs = []
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
            except:
                logs = []
        
        # æ·»åŠ æ–°æ—¥å¿—
        logs.append(performance_data)
        
        # ä¿å­˜æ—¥å¿—
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=2)
    
    def start_resource_monitoring(self, interval: float = 1.0):
        """
        å¯åŠ¨èµ„æºç›‘æ§
        
        Args:
            interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
        """
        
        if self.monitoring_active:
            print("âš ï¸  èµ„æºç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._resource_monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        print("ğŸ” èµ„æºç›‘æ§å·²å¯åŠ¨")
    
    def stop_resource_monitoring(self):
        """åœæ­¢èµ„æºç›‘æ§"""
        
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        print("â¹ï¸  èµ„æºç›‘æ§å·²åœæ­¢")
    
    def _resource_monitor_loop(self, interval: float):
        """èµ„æºç›‘æ§å¾ªç¯"""
        
        while self.monitoring_active:
            try:
                # è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                
                # è·å–è¿›ç¨‹èµ„æºä¿¡æ¯
                process = psutil.Process()
                process_memory = process.memory_info()
                process_cpu = process.cpu_percent()
                
                resource_data = {
                    'timestamp': datetime.now().isoformat(),
                    'system': {
                        'cpu_percent': cpu_percent,
                        'memory_percent': memory.percent,
                        'memory_used_mb': memory.used / 1024 / 1024,
                        'memory_available_mb': memory.available / 1024 / 1024,
                        'disk_percent': disk.percent
                    },
                    'process': {
                        'cpu_percent': process_cpu,
                        'memory_mb': process_memory.rss / 1024 / 1024,
                        'memory_percent': process_memory.rss / memory.total * 100
                    }
                }
                
                self.resource_logs.append(resource_data)
                
                # é™åˆ¶æ—¥å¿—æ•°é‡
                if len(self.resource_logs) > 1000:
                    self.resource_logs = self.resource_logs[-500:]
                
                time.sleep(interval)
                
            except Exception as e:
                print(f"èµ„æºç›‘æ§é”™è¯¯: {str(e)}")
                time.sleep(interval)
    
    def benchmark_model_performance(self, 
                                  models: Dict[str, Any],
                                  X_train: np.ndarray,
                                  X_test: np.ndarray,
                                  y_train: np.ndarray,
                                  y_test: np.ndarray,
                                  n_runs: int = 5) -> Dict:
        """
        æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            models: æ¨¡å‹å­—å…¸
            X_train: è®­ç»ƒæ•°æ®
            X_test: æµ‹è¯•æ•°æ®
            y_train: è®­ç»ƒæ ‡ç­¾
            y_test: æµ‹è¯•æ ‡ç­¾
            n_runs: è¿è¡Œæ¬¡æ•°
            
        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        
        print("ğŸƒâ€â™‚ï¸ å¼€å§‹æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        benchmark_results = {}
        
        for model_name, model in models.items():
            print(f"  ğŸ“Š æµ‹è¯•æ¨¡å‹: {model_name}")
            
            model_results = {
                'training_performance': [],
                'prediction_performance': [],
                'accuracy_scores': [],
                'resource_usage': []
            }
            
            for run in range(n_runs):
                print(f"    ğŸ”„ è¿è¡Œ {run + 1}/{n_runs}")
                
                # å…‹éš†æ¨¡å‹ä»¥ç¡®ä¿æ¯æ¬¡è¿è¡Œçš„ç‹¬ç«‹æ€§
                model_copy = clone(model)
                
                # è®­ç»ƒæ€§èƒ½æµ‹è¯•
                with self.monitor_performance(f"{model_name}_training_run_{run}"):
                    start_time = time.time()
                    model_copy.fit(X_train, y_train)
                    training_time = time.time() - start_time
                
                # é¢„æµ‹æ€§èƒ½æµ‹è¯•
                with self.monitor_performance(f"{model_name}_prediction_run_{run}"):
                    start_time = time.time()
                    y_pred = model_copy.predict(X_test)
                    prediction_time = time.time() - start_time
                
                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = accuracy_score(y_test, y_pred)
                
                # è®°å½•ç»“æœ
                model_results['training_performance'].append(training_time)
                model_results['prediction_performance'].append(prediction_time)
                model_results['accuracy_scores'].append(accuracy)
                
                # è·å–æœ€æ–°çš„æ€§èƒ½æ—¥å¿—
                if self.performance_logs:
                    latest_logs = self.performance_logs[-2:]  # è®­ç»ƒå’Œé¢„æµ‹çš„æ—¥å¿—
                    model_results['resource_usage'].extend(latest_logs)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            model_results['statistics'] = {
                'training_time': {
                    'mean': np.mean(model_results['training_performance']),
                    'std': np.std(model_results['training_performance']),
                    'min': np.min(model_results['training_performance']),
                    'max': np.max(model_results['training_performance'])
                },
                'prediction_time': {
                    'mean': np.mean(model_results['prediction_performance']),
                    'std': np.std(model_results['prediction_performance']),
                    'min': np.min(model_results['prediction_performance']),
                    'max': np.max(model_results['prediction_performance'])
                },
                'accuracy': {
                    'mean': np.mean(model_results['accuracy_scores']),
                    'std': np.std(model_results['accuracy_scores']),
                    'min': np.min(model_results['accuracy_scores']),
                    'max': np.max(model_results['accuracy_scores'])
                }
            }
            
            benchmark_results[model_name] = model_results
        
        print("âœ… æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        return benchmark_results
    
    def analyze_performance_trends(self, benchmark_results: Dict) -> Dict:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        
        trends = {
            'efficiency_ranking': {},
            'stability_ranking': {},
            'resource_efficiency': {},
            'performance_comparison': {}
        }
        
        try:
            # æ•ˆç‡æ’åï¼ˆè®­ç»ƒæ—¶é—´ + é¢„æµ‹æ—¶é—´ï¼‰
            efficiency_scores = {}
            for model_name, results in benchmark_results.items():
                stats = results['statistics']
                total_time = stats['training_time']['mean'] + stats['prediction_time']['mean']
                accuracy = stats['accuracy']['mean']
                
                # æ•ˆç‡è¯„åˆ† = å‡†ç¡®ç‡ / æ€»æ—¶é—´
                efficiency_scores[model_name] = accuracy / total_time if total_time > 0 else 0
            
            # æŒ‰æ•ˆç‡æ’åº
            sorted_efficiency = sorted(efficiency_scores.items(), key=lambda x: x[1], reverse=True)
            trends['efficiency_ranking'] = {
                'ranking': [{'model': model, 'score': score} for model, score in sorted_efficiency],
                'best_model': sorted_efficiency[0][0] if sorted_efficiency else None
            }
            
            # ç¨³å®šæ€§æ’åï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šç¨³å®šï¼‰
            stability_scores = {}
            for model_name, results in benchmark_results.items():
                stats = results['statistics']
                # ç»¼åˆç¨³å®šæ€§è¯„åˆ†ï¼ˆå‡†ç¡®ç‡ç¨³å®šæ€§ + æ—¶é—´ç¨³å®šæ€§ï¼‰
                acc_stability = 1 / (stats['accuracy']['std'] + 1e-6)
                time_stability = 1 / (stats['training_time']['std'] + stats['prediction_time']['std'] + 1e-6)
                
                stability_scores[model_name] = (acc_stability + time_stability) / 2
            
            sorted_stability = sorted(stability_scores.items(), key=lambda x: x[1], reverse=True)
            trends['stability_ranking'] = {
                'ranking': [{'model': model, 'score': score} for model, score in sorted_stability],
                'most_stable_model': sorted_stability[0][0] if sorted_stability else None
            }
            
            # èµ„æºæ•ˆç‡åˆ†æ
            for model_name, results in benchmark_results.items():
                if results['resource_usage']:
                    memory_usage = [log['process_memory']['delta_mb'] for log in results['resource_usage'] 
                                  if 'process_memory' in log]
                    
                    if memory_usage:
                        trends['resource_efficiency'][model_name] = {
                            'avg_memory_usage_mb': np.mean(memory_usage),
                            'max_memory_usage_mb': np.max(memory_usage),
                            'memory_efficiency_score': stats['accuracy']['mean'] / (np.mean(memory_usage) + 1e-6)
                        }
            
        except Exception as e:
            trends['error'] = str(e)
        
        return trends


class StabilityTester:
    """ç¨³å®šæ€§æµ‹è¯•å™¨"""
    
    def __init__(self, output_dir: str = "stability_testing"):
        """
        åˆå§‹åŒ–ç¨³å®šæ€§æµ‹è¯•å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.charts_dir = os.path.join(output_dir, "charts")
        self.reports_dir = os.path.join(output_dir, "reports")
        self.data_dir = os.path.join(output_dir, "data")
        
        for dir_path in [self.charts_dir, self.reports_dir, self.data_dir]:
            os.makedirs(dir_path, exist_ok=True)
    
    def comprehensive_stability_test(self, 
                                   models: Dict[str, Any],
                                   X_train: np.ndarray,
                                   X_test: np.ndarray,
                                   y_train: np.ndarray,
                                   y_test: np.ndarray,
                                   feature_names: List[str] = None) -> Dict:
        """
        ç»¼åˆç¨³å®šæ€§æµ‹è¯•
        
        Args:
            models: æ¨¡å‹å­—å…¸
            X_train: è®­ç»ƒæ•°æ®
            X_test: æµ‹è¯•æ•°æ®
            y_train: è®­ç»ƒæ ‡ç­¾
            y_test: æµ‹è¯•æ ‡ç­¾
            feature_names: ç‰¹å¾åç§°
            
        Returns:
            ç¨³å®šæ€§æµ‹è¯•ç»“æœ
        """
        
        print("ğŸ”¬ å¼€å§‹ç»¼åˆç¨³å®šæ€§æµ‹è¯•...")
        
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]
        
        stability_results = {}
        
        for model_name, model in models.items():
            print(f"  ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
            
            model_stability = {
                'data_distribution_stability': {},
                'noise_robustness': {},
                'feature_perturbation': {},
                'temporal_stability': {},
                'cross_validation_stability': {},
                'overall_stability_score': 0
            }
            
            try:
                # 1. æ•°æ®åˆ†å¸ƒç¨³å®šæ€§æµ‹è¯•
                model_stability['data_distribution_stability'] = self._test_data_distribution_stability(
                    model, X_train, X_test, y_train, y_test
                )
                
                # 2. å™ªå£°é²æ£’æ€§æµ‹è¯•
                model_stability['noise_robustness'] = self._test_noise_robustness(
                    model, X_train, X_test, y_train, y_test
                )
                
                # 3. ç‰¹å¾æ‰°åŠ¨æµ‹è¯•
                model_stability['feature_perturbation'] = self._test_feature_perturbation(
                    model, X_train, X_test, y_train, y_test, feature_names
                )
                
                # 4. æ—¶é—´ç¨³å®šæ€§æµ‹è¯•
                model_stability['temporal_stability'] = self._test_temporal_stability(
                    model, X_train, X_test, y_train, y_test
                )
                
                # 5. äº¤å‰éªŒè¯ç¨³å®šæ€§
                model_stability['cross_validation_stability'] = self._test_cross_validation_stability(
                    model, X_train, y_train
                )
                
                # è®¡ç®—ç»¼åˆç¨³å®šæ€§è¯„åˆ†
                model_stability['overall_stability_score'] = self._calculate_overall_stability_score(
                    model_stability
                )
                
            except Exception as e:
                model_stability['error'] = str(e)
                print(f"    âŒ {model_name} ç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {str(e)}")
            
            stability_results[model_name] = model_stability
        
        # ç”Ÿæˆç¨³å®šæ€§å¯¹æ¯”åˆ†æ
        stability_results['comparison'] = self._compare_model_stability(stability_results)
        
        print("âœ… ç»¼åˆç¨³å®šæ€§æµ‹è¯•å®Œæˆ")
        return stability_results
    
    def _test_data_distribution_stability(self, 
                                        model: Any,
                                        X_train: np.ndarray,
                                        X_test: np.ndarray,
                                        y_train: np.ndarray,
                                        y_test: np.ndarray) -> Dict:
        """æµ‹è¯•æ•°æ®åˆ†å¸ƒç¨³å®šæ€§"""
        
        try:
            # è®­ç»ƒæ¨¡å‹
            model_copy = clone(model)
            model_copy.fit(X_train, y_train)
            
            # åŸºå‡†æ€§èƒ½
            baseline_pred = model_copy.predict(X_test)
            baseline_accuracy = accuracy_score(y_test, baseline_pred)
            
            distribution_tests = []
            
            # æµ‹è¯•ä¸åŒçš„æ•°æ®åˆ†å¸ƒå˜åŒ–
            distribution_changes = [
                ('æ ‡å‡†åŒ–', lambda X: StandardScaler().fit_transform(X)),
                ('æ·»åŠ åç§»', lambda X: X + np.random.normal(0, 0.1, X.shape)),
                ('ç¼©æ”¾å˜æ¢', lambda X: X * np.random.uniform(0.8, 1.2, X.shape[1])),
                ('ç‰¹å¾é‡æ’', lambda X: X[:, np.random.permutation(X.shape[1])]),
            ]
            
            for change_name, transform_func in distribution_changes:
                try:
                    # åº”ç”¨å˜æ¢
                    X_test_transformed = transform_func(X_test.copy())
                    
                    # é¢„æµ‹
                    pred_transformed = model_copy.predict(X_test_transformed)
                    accuracy_transformed = accuracy_score(y_test, pred_transformed)
                    
                    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
                    accuracy_drop = baseline_accuracy - accuracy_transformed
                    stability_score = max(0, 1 - abs(accuracy_drop) / baseline_accuracy)
                    
                    distribution_tests.append({
                        'transformation': change_name,
                        'baseline_accuracy': baseline_accuracy,
                        'transformed_accuracy': accuracy_transformed,
                        'accuracy_drop': accuracy_drop,
                        'stability_score': stability_score
                    })
                    
                except Exception as e:
                    distribution_tests.append({
                        'transformation': change_name,
                        'error': str(e)
                    })
            
            # è®¡ç®—å¹³å‡ç¨³å®šæ€§
            valid_tests = [t for t in distribution_tests if 'error' not in t]
            avg_stability = np.mean([t['stability_score'] for t in valid_tests]) if valid_tests else 0
            
            return {
                'tests': distribution_tests,
                'average_stability_score': avg_stability,
                'most_stable_transformation': max(valid_tests, key=lambda x: x['stability_score'])['transformation'] if valid_tests else None,
                'least_stable_transformation': min(valid_tests, key=lambda x: x['stability_score'])['transformation'] if valid_tests else None
            }
            
        except Exception as e:
            return {'error': f'Data distribution stability test failed: {str(e)}'}
    
    def _test_noise_robustness(self, 
                             model: Any,
                             X_train: np.ndarray,
                             X_test: np.ndarray,
                             y_train: np.ndarray,
                             y_test: np.ndarray) -> Dict:
        """æµ‹è¯•å™ªå£°é²æ£’æ€§"""
        
        try:
            # è®­ç»ƒæ¨¡å‹
            model_copy = clone(model)
            model_copy.fit(X_train, y_train)
            
            # åŸºå‡†æ€§èƒ½
            baseline_pred = model_copy.predict(X_test)
            baseline_accuracy = accuracy_score(y_test, baseline_pred)
            
            noise_tests = []
            noise_levels = [0.01, 0.05, 0.1, 0.2, 0.3]
            
            for noise_level in noise_levels:
                try:
                    # æ·»åŠ é«˜æ–¯å™ªå£°
                    noise = np.random.normal(0, noise_level, X_test.shape)
                    X_test_noisy = X_test + noise
                    
                    # é¢„æµ‹
                    pred_noisy = model_copy.predict(X_test_noisy)
                    accuracy_noisy = accuracy_score(y_test, pred_noisy)
                    
                    # è®¡ç®—é²æ£’æ€§æŒ‡æ ‡
                    accuracy_drop = baseline_accuracy - accuracy_noisy
                    robustness_score = max(0, 1 - abs(accuracy_drop) / baseline_accuracy)
                    
                    # é¢„æµ‹ä¸€è‡´æ€§
                    consistency = np.mean(baseline_pred == pred_noisy)
                    
                    noise_tests.append({
                        'noise_level': noise_level,
                        'baseline_accuracy': baseline_accuracy,
                        'noisy_accuracy': accuracy_noisy,
                        'accuracy_drop': accuracy_drop,
                        'robustness_score': robustness_score,
                        'prediction_consistency': consistency
                    })
                    
                except Exception as e:
                    noise_tests.append({
                        'noise_level': noise_level,
                        'error': str(e)
                    })
            
            # è®¡ç®—å¹³å‡é²æ£’æ€§
            valid_tests = [t for t in noise_tests if 'error' not in t]
            avg_robustness = np.mean([t['robustness_score'] for t in valid_tests]) if valid_tests else 0
            avg_consistency = np.mean([t['prediction_consistency'] for t in valid_tests]) if valid_tests else 0
            
            return {
                'tests': noise_tests,
                'average_robustness_score': avg_robustness,
                'average_consistency': avg_consistency,
                'robustness_trend': self._analyze_robustness_trend(valid_tests)
            }
            
        except Exception as e:
            return {'error': f'Noise robustness test failed: {str(e)}'}
    
    def _analyze_robustness_trend(self, tests: List[Dict]) -> Dict:
        """åˆ†æé²æ£’æ€§è¶‹åŠ¿"""
        
        if len(tests) < 2:
            return {'error': 'Insufficient data for trend analysis'}
        
        noise_levels = [t['noise_level'] for t in tests]
        robustness_scores = [t['robustness_score'] for t in tests]
        
        # è®¡ç®—è¶‹åŠ¿æ–œç‡
        slope, intercept, r_value, p_value, std_err = stats.linregress(noise_levels, robustness_scores)
        
        return {
            'slope': slope,
            'r_squared': r_value ** 2,
            'trend_direction': 'decreasing' if slope < -0.1 else 'stable' if abs(slope) <= 0.1 else 'increasing',
            'degradation_rate': abs(slope) if slope < 0 else 0
        }
    
    def _test_feature_perturbation(self, 
                                 model: Any,
                                 X_train: np.ndarray,
                                 X_test: np.ndarray,
                                 y_train: np.ndarray,
                                 y_test: np.ndarray,
                                 feature_names: List[str]) -> Dict:
        """æµ‹è¯•ç‰¹å¾æ‰°åŠ¨ç¨³å®šæ€§"""
        
        try:
            # è®­ç»ƒæ¨¡å‹
            model_copy = clone(model)
            model_copy.fit(X_train, y_train)
            
            # åŸºå‡†æ€§èƒ½
            baseline_pred = model_copy.predict(X_test)
            baseline_accuracy = accuracy_score(y_test, baseline_pred)
            
            feature_tests = []
            
            # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œæ‰°åŠ¨æµ‹è¯•
            for i, feature_name in enumerate(feature_names):
                try:
                    X_test_perturbed = X_test.copy()
                    
                    # ç‰¹å¾æ‰°åŠ¨ç­–ç•¥
                    perturbation_methods = [
                        ('é›¶å€¼æ›¿æ¢', lambda x: np.zeros_like(x)),
                        ('å‡å€¼æ›¿æ¢', lambda x: np.full_like(x, np.mean(x))),
                        ('éšæœºå™ªå£°', lambda x: x + np.random.normal(0, np.std(x) * 0.1, x.shape)),
                        ('éšæœºæ‰“ä¹±', lambda x: np.random.permutation(x))
                    ]
                    
                    feature_perturbation_results = []
                    
                    for method_name, perturbation_func in perturbation_methods:
                        X_test_perturbed[:, i] = perturbation_func(X_test[:, i])
                        
                        pred_perturbed = model_copy.predict(X_test_perturbed)
                        accuracy_perturbed = accuracy_score(y_test, pred_perturbed)
                        
                        accuracy_drop = baseline_accuracy - accuracy_perturbed
                        sensitivity = abs(accuracy_drop) / baseline_accuracy
                        
                        feature_perturbation_results.append({
                            'method': method_name,
                            'accuracy_drop': accuracy_drop,
                            'sensitivity': sensitivity
                        })
                    
                    # è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºæ‰°åŠ¨æ•æ„Ÿæ€§ï¼‰
                    avg_sensitivity = np.mean([r['sensitivity'] for r in feature_perturbation_results])
                    
                    feature_tests.append({
                        'feature_name': feature_name,
                        'feature_index': i,
                        'perturbation_results': feature_perturbation_results,
                        'average_sensitivity': avg_sensitivity,
                        'importance_rank': 0  # å°†åœ¨åé¢è®¡ç®—
                    })
                    
                except Exception as e:
                    feature_tests.append({
                        'feature_name': feature_name,
                        'feature_index': i,
                        'error': str(e)
                    })
            
            # è®¡ç®—é‡è¦æ€§æ’å
            valid_tests = [t for t in feature_tests if 'error' not in t]
            if valid_tests:
                sorted_tests = sorted(valid_tests, key=lambda x: x['average_sensitivity'], reverse=True)
                for rank, test in enumerate(sorted_tests):
                    test['importance_rank'] = rank + 1
            
            return {
                'feature_tests': feature_tests,
                'most_sensitive_features': sorted(valid_tests, key=lambda x: x['average_sensitivity'], reverse=True)[:5] if valid_tests else [],
                'least_sensitive_features': sorted(valid_tests, key=lambda x: x['average_sensitivity'])[:5] if valid_tests else [],
                'average_feature_sensitivity': np.mean([t['average_sensitivity'] for t in valid_tests]) if valid_tests else 0
            }
            
        except Exception as e:
            return {'error': f'Feature perturbation test failed: {str(e)}'}
    
    def _test_temporal_stability(self, 
                               model: Any,
                               X_train: np.ndarray,
                               X_test: np.ndarray,
                               y_train: np.ndarray,
                               y_test: np.ndarray) -> Dict:
        """æµ‹è¯•æ—¶é—´ç¨³å®šæ€§"""
        
        try:
            # æ¨¡æ‹Ÿæ—¶é—´åºåˆ—ç¨³å®šæ€§æµ‹è¯•
            temporal_results = []
            n_time_splits = 5
            
            # å°†æµ‹è¯•æ•°æ®åˆ†æˆæ—¶é—´æ®µ
            split_size = len(X_test) // n_time_splits
            
            # è®­ç»ƒæ¨¡å‹
            model_copy = clone(model)
            model_copy.fit(X_train, y_train)
            
            for i in range(n_time_splits):
                start_idx = i * split_size
                end_idx = (i + 1) * split_size if i < n_time_splits - 1 else len(X_test)
                
                X_time_split = X_test[start_idx:end_idx]
                y_time_split = y_test[start_idx:end_idx]
                
                if len(X_time_split) > 0:
                    pred_time_split = model_copy.predict(X_time_split)
                    accuracy_time_split = accuracy_score(y_time_split, pred_time_split)
                    
                    temporal_results.append({
                        'time_period': i + 1,
                        'start_idx': start_idx,
                        'end_idx': end_idx,
                        'accuracy': accuracy_time_split,
                        'sample_count': len(X_time_split)
                    })
            
            # è®¡ç®—æ—¶é—´ç¨³å®šæ€§æŒ‡æ ‡
            accuracies = [r['accuracy'] for r in temporal_results]
            
            temporal_stability = {
                'time_periods': temporal_results,
                'accuracy_variance': np.var(accuracies),
                'accuracy_std': np.std(accuracies),
                'min_accuracy': np.min(accuracies),
                'max_accuracy': np.max(accuracies),
                'accuracy_range': np.max(accuracies) - np.min(accuracies),
                'stability_score': 1 - (np.std(accuracies) / np.mean(accuracies)) if np.mean(accuracies) > 0 else 0
            }
            
            return temporal_stability
            
        except Exception as e:
            return {'error': f'Temporal stability test failed: {str(e)}'}
    
    def _test_cross_validation_stability(self, 
                                       model: Any,
                                       X_train: np.ndarray,
                                       y_train: np.ndarray) -> Dict:
        """æµ‹è¯•äº¤å‰éªŒè¯ç¨³å®šæ€§"""
        
        try:
            from sklearn.model_selection import cross_val_score, StratifiedKFold
            
            # ä½¿ç”¨åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            
            # å¤šæ¬¡è¿è¡Œäº¤å‰éªŒè¯ä»¥æµ‹è¯•ç¨³å®šæ€§
            cv_runs = []
            n_runs = 3
            
            for run in range(n_runs):
                cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                cv_runs.append({
                    'run': run + 1,
                    'scores': cv_scores.tolist(),
                    'mean_score': np.mean(cv_scores),
                    'std_score': np.std(cv_scores)
                })
            
            # è®¡ç®—è·¨è¿è¡Œçš„ç¨³å®šæ€§
            mean_scores = [run['mean_score'] for run in cv_runs]
            std_scores = [run['std_score'] for run in cv_runs]
            
            cv_stability = {
                'cv_runs': cv_runs,
                'inter_run_stability': {
                    'mean_score_variance': np.var(mean_scores),
                    'mean_score_std': np.std(mean_scores),
                    'avg_intra_run_std': np.mean(std_scores),
                    'stability_score': 1 - (np.std(mean_scores) / np.mean(mean_scores)) if np.mean(mean_scores) > 0 else 0
                },
                'overall_cv_performance': {
                    'grand_mean': np.mean(mean_scores),
                    'grand_std': np.mean(std_scores),
                    'best_run': max(cv_runs, key=lambda x: x['mean_score'])['run'],
                    'worst_run': min(cv_runs, key=lambda x: x['mean_score'])['run']
                }
            }
            
            return cv_stability
            
        except Exception as e:
            return {'error': f'Cross-validation stability test failed: {str(e)}'}
    
    def _calculate_overall_stability_score(self, model_stability: Dict) -> float:
        """è®¡ç®—ç»¼åˆç¨³å®šæ€§è¯„åˆ†"""
        
        try:
            scores = []
            weights = []
            
            # æ•°æ®åˆ†å¸ƒç¨³å®šæ€§ (æƒé‡: 0.25)
            if ('data_distribution_stability' in model_stability and 
                'average_stability_score' in model_stability['data_distribution_stability']):
                scores.append(model_stability['data_distribution_stability']['average_stability_score'])
                weights.append(0.25)
            
            # å™ªå£°é²æ£’æ€§ (æƒé‡: 0.25)
            if ('noise_robustness' in model_stability and 
                'average_robustness_score' in model_stability['noise_robustness']):
                scores.append(model_stability['noise_robustness']['average_robustness_score'])
                weights.append(0.25)
            
            # ç‰¹å¾æ‰°åŠ¨ç¨³å®šæ€§ (æƒé‡: 0.2)
            if ('feature_perturbation' in model_stability and 
                'average_feature_sensitivity' in model_stability['feature_perturbation']):
                # æ•æ„Ÿæ€§è¶Šä½ï¼Œç¨³å®šæ€§è¶Šé«˜
                sensitivity = model_stability['feature_perturbation']['average_feature_sensitivity']
                stability = max(0, 1 - sensitivity)
                scores.append(stability)
                weights.append(0.2)
            
            # æ—¶é—´ç¨³å®šæ€§ (æƒé‡: 0.15)
            if ('temporal_stability' in model_stability and 
                'stability_score' in model_stability['temporal_stability']):
                scores.append(model_stability['temporal_stability']['stability_score'])
                weights.append(0.15)
            
            # äº¤å‰éªŒè¯ç¨³å®šæ€§ (æƒé‡: 0.15)
            if ('cross_validation_stability' in model_stability and 
                'inter_run_stability' in model_stability['cross_validation_stability']):
                cv_stability = model_stability['cross_validation_stability']['inter_run_stability']['stability_score']
                scores.append(cv_stability)
                weights.append(0.15)
            
            # è®¡ç®—åŠ æƒå¹³å‡
            if scores and weights:
                # å½’ä¸€åŒ–æƒé‡
                total_weight = sum(weights)
                normalized_weights = [w / total_weight for w in weights]
                
                overall_score = sum(s * w for s, w in zip(scores, normalized_weights))
                return max(0, min(1, overall_score))  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
            else:
                return 0.0
                
        except Exception as e:
            print(f"è®¡ç®—ç»¼åˆç¨³å®šæ€§è¯„åˆ†å¤±è´¥: {str(e)}")
            return 0.0
    
    def _compare_model_stability(self, stability_results: Dict) -> Dict:
        """å¯¹æ¯”æ¨¡å‹ç¨³å®šæ€§"""
        
        comparison = {
            'stability_ranking': [],
            'stability_categories': {},
            'best_performers': {},
            'recommendations': []
        }
        
        try:
            # æå–æœ‰æ•ˆçš„ç¨³å®šæ€§è¯„åˆ†
            model_scores = {}
            for model_name, results in stability_results.items():
                if model_name != 'comparison' and 'overall_stability_score' in results:
                    model_scores[model_name] = results['overall_stability_score']
            
            # ç¨³å®šæ€§æ’å
            sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
            comparison['stability_ranking'] = [
                {'model': model, 'stability_score': score, 'rank': i + 1}
                for i, (model, score) in enumerate(sorted_models)
            ]
            
            # ç¨³å®šæ€§åˆ†ç±»
            for model, score in model_scores.items():
                if score >= 0.8:
                    category = 'é«˜ç¨³å®šæ€§'
                elif score >= 0.6:
                    category = 'ä¸­ç­‰ç¨³å®šæ€§'
                elif score >= 0.4:
                    category = 'ä½ç¨³å®šæ€§'
                else:
                    category = 'ä¸ç¨³å®š'
                
                if category not in comparison['stability_categories']:
                    comparison['stability_categories'][category] = []
                comparison['stability_categories'][category].append(model)
            
            # å„ç»´åº¦æœ€ä½³è¡¨ç°è€…
            dimensions = [
                ('data_distribution_stability', 'average_stability_score', 'æ•°æ®åˆ†å¸ƒç¨³å®šæ€§'),
                ('noise_robustness', 'average_robustness_score', 'å™ªå£°é²æ£’æ€§'),
                ('temporal_stability', 'stability_score', 'æ—¶é—´ç¨³å®šæ€§')
            ]
            
            for dim_key, score_key, dim_name in dimensions:
                best_model = None
                best_score = -1
                
                for model_name, results in stability_results.items():
                    if (model_name != 'comparison' and 
                        dim_key in results and 
                        score_key in results[dim_key]):
                        score = results[dim_key][score_key]
                        if score > best_score:
                            best_score = score
                            best_model = model_name
                
                if best_model:
                    comparison['best_performers'][dim_name] = {
                        'model': best_model,
                        'score': best_score
                    }
            
            # ç”Ÿæˆå»ºè®®
            if sorted_models:
                best_model = sorted_models[0][0]
                worst_model = sorted_models[-1][0]
                
                comparison['recommendations'].extend([
                    f"æ¨èä½¿ç”¨ {best_model}ï¼Œå…·æœ‰æœ€é«˜çš„ç¨³å®šæ€§è¯„åˆ† ({sorted_models[0][1]:.3f})",
                    f"éœ€è¦æ”¹è¿› {worst_model} çš„ç¨³å®šæ€§ ({sorted_models[-1][1]:.3f})"
                ])
                
                # åŸºäºç¨³å®šæ€§åˆ†ç±»çš„å»ºè®®
                if 'ä¸ç¨³å®š' in comparison['stability_categories']:
                    unstable_models = comparison['stability_categories']['ä¸ç¨³å®š']
                    comparison['recommendations'].append(
                        f"ä»¥ä¸‹æ¨¡å‹ç¨³å®šæ€§è¾ƒå·®ï¼Œå»ºè®®é‡æ–°è°ƒå‚æˆ–æ›´æ¢ç®—æ³•: {', '.join(unstable_models)}"
                    )
                
                if 'é«˜ç¨³å®šæ€§' in comparison['stability_categories']:
                    stable_models = comparison['stability_categories']['é«˜ç¨³å®šæ€§']
                    comparison['recommendations'].append(
                        f"ä»¥ä¸‹æ¨¡å‹è¡¨ç°ç¨³å®šï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒéƒ¨ç½²: {', '.join(stable_models)}"
                    )
            
        except Exception as e:
            comparison['error'] = str(e)
        
        return comparison


class PerformanceStabilitySystem:
    """æ€§èƒ½ç›‘æ§å’Œç¨³å®šæ€§æµ‹è¯•é›†æˆç³»ç»Ÿ"""
    
    def __init__(self, output_dir: str = "performance_stability_system"):
        """
        åˆå§‹åŒ–æ€§èƒ½ç¨³å®šæ€§ç³»ç»Ÿ
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–å­ç³»ç»Ÿ
        self.performance_monitor = PerformanceMonitor(
            os.path.join(output_dir, "performance_monitoring")
        )
        self.stability_tester = StabilityTester(
            os.path.join(output_dir, "stability_testing")
        )
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        self.reports_dir = os.path.join(output_dir, "reports")
        self.charts_dir = os.path.join(output_dir, "charts")
        
        for dir_path in [self.reports_dir, self.charts_dir]:
            os.makedirs(dir_path, exist_ok=True)
    
    def comprehensive_analysis(self, 
                             models: Dict[str, Any],
                             X_train: np.ndarray,
                             X_test: np.ndarray,
                             y_train: np.ndarray,
                             y_test: np.ndarray,
                             feature_names: List[str] = None) -> Dict:
        """
        ç»¼åˆæ€§èƒ½å’Œç¨³å®šæ€§åˆ†æ
        
        Args:
            models: æ¨¡å‹å­—å…¸
            X_train: è®­ç»ƒæ•°æ®
            X_test: æµ‹è¯•æ•°æ®
            y_train: è®­ç»ƒæ ‡ç­¾
            y_test: æµ‹è¯•æ ‡ç­¾
            feature_names: ç‰¹å¾åç§°
            
        Returns:
            ç»¼åˆåˆ†æç»“æœ
        """
        
        print("ğŸš€ å¼€å§‹ç»¼åˆæ€§èƒ½å’Œç¨³å®šæ€§åˆ†æ...")
        
        # å¯åŠ¨èµ„æºç›‘æ§
        self.performance_monitor.start_resource_monitoring()
        
        try:
            results = {
                'performance_analysis': {},
                'stability_analysis': {},
                'integrated_analysis': {},
                'recommendations': [],
                'charts': {}
            }
            
            # æ€§èƒ½åŸºå‡†æµ‹è¯•
            print("ğŸ“Š æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
            benchmark_results = self.performance_monitor.benchmark_model_performance(
                models, X_train, X_test, y_train, y_test
            )
            results['performance_analysis']['benchmark'] = benchmark_results
            
            # æ€§èƒ½è¶‹åŠ¿åˆ†æ
            print("ğŸ“ˆ åˆ†ææ€§èƒ½è¶‹åŠ¿...")
            performance_trends = self.performance_monitor.analyze_performance_trends(benchmark_results)
            results['performance_analysis']['trends'] = performance_trends
            
            # ç¨³å®šæ€§æµ‹è¯•
            print("ğŸ”¬ æ‰§è¡Œç¨³å®šæ€§æµ‹è¯•...")
            stability_results = self.stability_tester.comprehensive_stability_test(
                models, X_train, X_test, y_train, y_test, feature_names
            )
            results['stability_analysis'] = stability_results
            
            # é›†æˆåˆ†æ
            print("ğŸ”„ æ‰§è¡Œé›†æˆåˆ†æ...")
            results['integrated_analysis'] = self._integrated_analysis(
                benchmark_results, performance_trends, stability_results
            )
            
            # ç”Ÿæˆå»ºè®®
            results['recommendations'] = self._generate_comprehensive_recommendations(results)
            
            # ç”Ÿæˆå›¾è¡¨
            results['charts'] = self._generate_comprehensive_charts(results)
            
            print("âœ… ç»¼åˆåˆ†æå®Œæˆ")
            return results
            
        finally:
            # åœæ­¢èµ„æºç›‘æ§
            self.performance_monitor.stop_resource_monitoring()
    
    def _integrated_analysis(self, 
                           benchmark_results: Dict,
                           performance_trends: Dict,
                           stability_results: Dict) -> Dict:
        """é›†æˆåˆ†æ"""
        
        integrated = {
            'model_rankings': {},
            'performance_stability_matrix': {},
            'optimal_model_selection': {},
            'trade_off_analysis': {}
        }
        
        try:
            # æå–æ¨¡å‹è¯„åˆ†
            model_scores = {}
            
            for model_name in benchmark_results.keys():
                scores = {
                    'performance_score': 0,
                    'stability_score': 0,
                    'efficiency_score': 0,
                    'overall_score': 0
                }
                
                # æ€§èƒ½è¯„åˆ†ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰
                if 'statistics' in benchmark_results[model_name]:
                    accuracy = benchmark_results[model_name]['statistics']['accuracy']['mean']
                    scores['performance_score'] = accuracy
                
                # ç¨³å®šæ€§è¯„åˆ†
                if (model_name in stability_results and 
                    'overall_stability_score' in stability_results[model_name]):
                    scores['stability_score'] = stability_results[model_name]['overall_stability_score']
                
                # æ•ˆç‡è¯„åˆ†
                if ('efficiency_ranking' in performance_trends and 
                    'ranking' in performance_trends['efficiency_ranking']):
                    for rank_info in performance_trends['efficiency_ranking']['ranking']:
                        if rank_info['model'] == model_name:
                            # å½’ä¸€åŒ–æ•ˆç‡è¯„åˆ†
                            max_score = max([r['score'] for r in performance_trends['efficiency_ranking']['ranking']])
                            scores['efficiency_score'] = rank_info['score'] / max_score if max_score > 0 else 0
                            break
                
                # ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
                weights = {'performance': 0.4, 'stability': 0.4, 'efficiency': 0.2}
                scores['overall_score'] = (
                    scores['performance_score'] * weights['performance'] +
                    scores['stability_score'] * weights['stability'] +
                    scores['efficiency_score'] * weights['efficiency']
                )
                
                model_scores[model_name] = scores
            
            # æ¨¡å‹æ’å
            integrated['model_rankings'] = {
                'by_performance': sorted(model_scores.items(), key=lambda x: x[1]['performance_score'], reverse=True),
                'by_stability': sorted(model_scores.items(), key=lambda x: x[1]['stability_score'], reverse=True),
                'by_efficiency': sorted(model_scores.items(), key=lambda x: x[1]['efficiency_score'], reverse=True),
                'by_overall': sorted(model_scores.items(), key=lambda x: x[1]['overall_score'], reverse=True)
            }
            
            # æ€§èƒ½-ç¨³å®šæ€§çŸ©é˜µ
            integrated['performance_stability_matrix'] = self._create_performance_stability_matrix(model_scores)
            
            # æœ€ä¼˜æ¨¡å‹é€‰æ‹©
            integrated['optimal_model_selection'] = self._select_optimal_models(model_scores)
            
            # æƒè¡¡åˆ†æ
            integrated['trade_off_analysis'] = self._analyze_trade_offs(model_scores)
            
        except Exception as e:
            integrated['error'] = str(e)
        
        return integrated
    
    def _create_performance_stability_matrix(self, model_scores: Dict) -> Dict:
        """åˆ›å»ºæ€§èƒ½-ç¨³å®šæ€§çŸ©é˜µ"""
        
        matrix = {
            'high_performance_high_stability': [],
            'high_performance_low_stability': [],
            'low_performance_high_stability': [],
            'low_performance_low_stability': []
        }
        
        # è®¡ç®—é˜ˆå€¼ï¼ˆä¸­ä½æ•°ï¼‰
        performance_scores = [scores['performance_score'] for scores in model_scores.values()]
        stability_scores = [scores['stability_score'] for scores in model_scores.values()]
        
        perf_threshold = np.median(performance_scores) if performance_scores else 0.5
        stab_threshold = np.median(stability_scores) if stability_scores else 0.5
        
        for model_name, scores in model_scores.items():
            perf = scores['performance_score']
            stab = scores['stability_score']
            
            if perf >= perf_threshold and stab >= stab_threshold:
                matrix['high_performance_high_stability'].append(model_name)
            elif perf >= perf_threshold and stab < stab_threshold:
                matrix['high_performance_low_stability'].append(model_name)
            elif perf < perf_threshold and stab >= stab_threshold:
                matrix['low_performance_high_stability'].append(model_name)
            else:
                matrix['low_performance_low_stability'].append(model_name)
        
        return matrix
    
    def _select_optimal_models(self, model_scores: Dict) -> Dict:
        """é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        
        selection = {}
        
        # ç»¼åˆæœ€ä¼˜
        best_overall = max(model_scores.items(), key=lambda x: x[1]['overall_score'])
        selection['best_overall'] = {
            'model': best_overall[0],
            'scores': best_overall[1]
        }
        
        # æ€§èƒ½æœ€ä¼˜
        best_performance = max(model_scores.items(), key=lambda x: x[1]['performance_score'])
        selection['best_performance'] = {
            'model': best_performance[0],
            'scores': best_performance[1]
        }
        
        # ç¨³å®šæ€§æœ€ä¼˜
        best_stability = max(model_scores.items(), key=lambda x: x[1]['stability_score'])
        selection['best_stability'] = {
            'model': best_stability[0],
            'scores': best_stability[1]
        }
        
        # æ•ˆç‡æœ€ä¼˜
        best_efficiency = max(model_scores.items(), key=lambda x: x[1]['efficiency_score'])
        selection['best_efficiency'] = {
            'model': best_efficiency[0],
            'scores': best_efficiency[1]
        }
        
        return selection
    
    def _analyze_trade_offs(self, model_scores: Dict) -> Dict:
        """åˆ†ææƒè¡¡å…³ç³»"""
        
        trade_offs = {
            'performance_vs_stability': {},
            'performance_vs_efficiency': {},
            'stability_vs_efficiency': {},
            'correlations': {}
        }
        
        try:
            # æå–è¯„åˆ†æ•°ç»„
            models = list(model_scores.keys())
            performance = [model_scores[m]['performance_score'] for m in models]
            stability = [model_scores[m]['stability_score'] for m in models]
            efficiency = [model_scores[m]['efficiency_score'] for m in models]
            
            # è®¡ç®—ç›¸å…³æ€§
            if len(models) > 1:
                trade_offs['correlations'] = {
                    'performance_stability': float(np.corrcoef(performance, stability)[0, 1]) if len(set(performance)) > 1 and len(set(stability)) > 1 else 0,
                    'performance_efficiency': float(np.corrcoef(performance, efficiency)[0, 1]) if len(set(performance)) > 1 and len(set(efficiency)) > 1 else 0,
                    'stability_efficiency': float(np.corrcoef(stability, efficiency)[0, 1]) if len(set(stability)) > 1 and len(set(efficiency)) > 1 else 0
                }
            
            # æƒè¡¡åˆ†æ
            trade_offs['performance_vs_stability']['analysis'] = self._analyze_dimension_trade_off(
                models, performance, stability, 'performance', 'stability'
            )
            
            trade_offs['performance_vs_efficiency']['analysis'] = self._analyze_dimension_trade_off(
                models, performance, efficiency, 'performance', 'efficiency'
            )
            
            trade_offs['stability_vs_efficiency']['analysis'] = self._analyze_dimension_trade_off(
                models, stability, efficiency, 'stability', 'efficiency'
            )
            
        except Exception as e:
            trade_offs['error'] = str(e)
        
        return trade_offs
    
    def _analyze_dimension_trade_off(self, models: List[str], dim1_scores: List[float], 
                                   dim2_scores: List[float], dim1_name: str, dim2_name: str) -> Dict:
        """åˆ†æä¸¤ä¸ªç»´åº¦ä¹‹é—´çš„æƒè¡¡"""
        
        analysis = {
            'best_balance': None,
            'extreme_cases': {},
            'trade_off_strength': 0
        }
        
        try:
            # æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ï¼ˆä¸¤ä¸ªç»´åº¦çš„ä¹˜ç§¯æœ€å¤§ï¼‰
            balance_scores = [d1 * d2 for d1, d2 in zip(dim1_scores, dim2_scores)]
            best_balance_idx = np.argmax(balance_scores)
            
            analysis['best_balance'] = {
                'model': models[best_balance_idx],
                f'{dim1_name}_score': dim1_scores[best_balance_idx],
                f'{dim2_name}_score': dim2_scores[best_balance_idx],
                'balance_score': balance_scores[best_balance_idx]
            }
            
            # æç«¯æƒ…å†µ
            analysis['extreme_cases'] = {
                f'high_{dim1_name}_low_{dim2_name}': {
                    'models': [models[i] for i in range(len(models)) 
                             if dim1_scores[i] > np.median(dim1_scores) and dim2_scores[i] < np.median(dim2_scores)]
                },
                f'low_{dim1_name}_high_{dim2_name}': {
                    'models': [models[i] for i in range(len(models)) 
                             if dim1_scores[i] < np.median(dim1_scores) and dim2_scores[i] > np.median(dim2_scores)]
                }
            }
            
            # æƒè¡¡å¼ºåº¦ï¼ˆè´Ÿç›¸å…³æ€§çš„å¼ºåº¦ï¼‰
            if len(set(dim1_scores)) > 1 and len(set(dim2_scores)) > 1:
                correlation = np.corrcoef(dim1_scores, dim2_scores)[0, 1]
                analysis['trade_off_strength'] = max(0, -correlation)  # è´Ÿç›¸å…³è¡¨ç¤ºæƒè¡¡
            
        except Exception as e:
            analysis['error'] = str(e)
        
        return analysis
    
    def _generate_comprehensive_recommendations(self, results: Dict) -> List[str]:
        """ç”Ÿæˆç»¼åˆå»ºè®®"""
        
        recommendations = []
        
        try:
            # åŸºäºé›†æˆåˆ†æçš„å»ºè®®
            if 'integrated_analysis' in results and 'optimal_model_selection' in results['integrated_analysis']:
                selection = results['integrated_analysis']['optimal_model_selection']
                
                if 'best_overall' in selection:
                    best_model = selection['best_overall']['model']
                    recommendations.append(f"ğŸ† ç»¼åˆæ¨èæ¨¡å‹: {best_model}ï¼ˆç»¼åˆè¯„åˆ†æœ€é«˜ï¼‰")
                
                if 'best_performance' in selection and 'best_stability' in selection:
                    perf_model = selection['best_performance']['model']
                    stab_model = selection['best_stability']['model']
                    
                    if perf_model != stab_model:
                        recommendations.append(f"âš–ï¸  æ€§èƒ½ä¸ç¨³å®šæ€§æƒè¡¡: {perf_model}ï¼ˆé«˜æ€§èƒ½ï¼‰vs {stab_model}ï¼ˆé«˜ç¨³å®šæ€§ï¼‰")
            
            # åŸºäºæ€§èƒ½-ç¨³å®šæ€§çŸ©é˜µçš„å»ºè®®
            if ('integrated_analysis' in results and 
                'performance_stability_matrix' in results['integrated_analysis']):
                matrix = results['integrated_analysis']['performance_stability_matrix']
                
                if matrix['high_performance_high_stability']:
                    models = ', '.join(matrix['high_performance_high_stability'])
                    recommendations.append(f"âœ… ç†æƒ³é€‰æ‹©: {models}ï¼ˆé«˜æ€§èƒ½é«˜ç¨³å®šæ€§ï¼‰")
                
                if matrix['high_performance_low_stability']:
                    models = ', '.join(matrix['high_performance_low_stability'])
                    recommendations.append(f"âš ï¸  éœ€è¦ç¨³å®šæ€§æ”¹è¿›: {models}ï¼ˆé«˜æ€§èƒ½ä½†ç¨³å®šæ€§ä¸è¶³ï¼‰")
                
                if matrix['low_performance_high_stability']:
                    models = ', '.join(matrix['low_performance_high_stability'])
                    recommendations.append(f"ğŸ”§ éœ€è¦æ€§èƒ½ä¼˜åŒ–: {models}ï¼ˆç¨³å®šä½†æ€§èƒ½ä¸è¶³ï¼‰")
            
            # åŸºäºæƒè¡¡åˆ†æçš„å»ºè®®
            if ('integrated_analysis' in results and 
                'trade_off_analysis' in results['integrated_analysis'] and
                'correlations' in results['integrated_analysis']['trade_off_analysis']):
                
                correlations = results['integrated_analysis']['trade_off_analysis']['correlations']
                
                if correlations.get('performance_stability', 0) < -0.5:
                    recommendations.append("ğŸ“Š å‘ç°æ€§èƒ½ä¸ç¨³å®šæ€§å­˜åœ¨æ˜æ˜¾æƒè¡¡ï¼Œéœ€è¦æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©")
                
                if correlations.get('performance_efficiency', 0) < -0.5:
                    recommendations.append("âš¡ å‘ç°æ€§èƒ½ä¸æ•ˆç‡å­˜åœ¨æƒè¡¡ï¼Œé«˜æ€§èƒ½æ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šèµ„æº")
            
            # åŸºäºç¨³å®šæ€§æµ‹è¯•çš„å»ºè®®
            if ('stability_analysis' in results and 
                'comparison' in results['stability_analysis'] and
                'recommendations' in results['stability_analysis']['comparison']):
                
                stability_recommendations = results['stability_analysis']['comparison']['recommendations']
                recommendations.extend([f"ğŸ”¬ {rec}" for rec in stability_recommendations])
            
            # é€šç”¨å»ºè®®
            recommendations.extend([
                "ğŸ“ˆ å»ºè®®å®šæœŸç›‘æ§æ¨¡å‹æ€§èƒ½ï¼ŒåŠæ—¶å‘ç°æ€§èƒ½é€€åŒ–",
                "ğŸ”„ å»ºè®®å»ºç«‹æ¨¡å‹æ›´æ–°æœºåˆ¶ï¼Œåº”å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–",
                "ğŸ“Š å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æŒç»­æ”¶é›†æ€§èƒ½å’Œç¨³å®šæ€§æŒ‡æ ‡",
                "ğŸ›¡ï¸  å»ºè®®ä¸ºå…³é”®ä¸šåŠ¡åœºæ™¯é€‰æ‹©é«˜ç¨³å®šæ€§æ¨¡å‹"
            ])
            
        except Exception as e:
            recommendations.append(f"âŒ å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        return recommendations
    
    def _generate_comprehensive_charts(self, results: Dict) -> Dict:
        """ç”Ÿæˆç»¼åˆå›¾è¡¨"""
        
        charts = {}
        
        try:
            # æ€§èƒ½å¯¹æ¯”å›¾è¡¨
            charts['performance_comparison'] = self._create_performance_comparison_chart(results)
            
            # ç¨³å®šæ€§é›·è¾¾å›¾
            charts['stability_radar'] = self._create_stability_radar_chart(results)
            
            # æ€§èƒ½-ç¨³å®šæ€§æ•£ç‚¹å›¾
            charts['performance_stability_scatter'] = self._create_performance_stability_scatter(results)
            
            # èµ„æºä½¿ç”¨è¶‹åŠ¿å›¾
            charts['resource_usage_trend'] = self._create_resource_usage_trend_chart()
            
        except Exception as e:
            charts['error'] = str(e)
        
        return charts
    
    def _create_performance_comparison_chart(self, results: Dict) -> str:
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        
        try:
            if ('performance_analysis' not in results or 
                'benchmark' not in results['performance_analysis']):
                return None
            
            benchmark_data = results['performance_analysis']['benchmark']
            
            models = list(benchmark_data.keys())
            training_times = [benchmark_data[model]['statistics']['training_time']['mean'] 
                            for model in models]
            prediction_times = [benchmark_data[model]['statistics']['prediction_time']['mean'] 
                              for model in models]
            accuracies = [benchmark_data[model]['statistics']['accuracy']['mean'] 
                         for model in models]
            
            # åˆ›å»ºå­å›¾
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=('è®­ç»ƒæ—¶é—´å¯¹æ¯”', 'é¢„æµ‹æ—¶é—´å¯¹æ¯”', 'å‡†ç¡®ç‡å¯¹æ¯”', 'æ•ˆç‡ç»¼åˆè¯„åˆ†'),
                specs=[[{"secondary_y": False}, {"secondary_y": False}],
                       [{"secondary_y": False}, {"secondary_y": False}]]
            )
            
            # è®­ç»ƒæ—¶é—´
            fig.add_trace(
                go.Bar(x=models, y=training_times, name='è®­ç»ƒæ—¶é—´(ç§’)', 
                      marker_color='lightblue'),
                row=1, col=1
            )
            
            # é¢„æµ‹æ—¶é—´
            fig.add_trace(
                go.Bar(x=models, y=prediction_times, name='é¢„æµ‹æ—¶é—´(ç§’)', 
                      marker_color='lightgreen'),
                row=1, col=2
            )
            
            # å‡†ç¡®ç‡
            fig.add_trace(
                go.Bar(x=models, y=accuracies, name='å‡†ç¡®ç‡', 
                      marker_color='lightcoral'),
                row=2, col=1
            )
            
            # æ•ˆç‡è¯„åˆ†ï¼ˆå‡†ç¡®ç‡/æ€»æ—¶é—´ï¼‰
            efficiency_scores = [acc / (train_t + pred_t) if (train_t + pred_t) > 0 else 0
                               for acc, train_t, pred_t in zip(accuracies, training_times, prediction_times)]
            
            fig.add_trace(
                go.Bar(x=models, y=efficiency_scores, name='æ•ˆç‡è¯„åˆ†', 
                      marker_color='gold'),
                row=2, col=2
            )
            
            fig.update_layout(
                title_text="æ¨¡å‹æ€§èƒ½ç»¼åˆå¯¹æ¯”",
                showlegend=False,
                height=600
            )
            
            chart_path = os.path.join(self.charts_dir, "performance_comparison.html")
            fig.write_html(chart_path)
            
            return chart_path
            
        except Exception as e:
            print(f"åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨å¤±è´¥: {str(e)}")
            return None
    
    def _create_stability_radar_chart(self, results: Dict) -> str:
        """åˆ›å»ºç¨³å®šæ€§é›·è¾¾å›¾"""
        
        try:
            if ('stability_analysis' not in results):
                return None
            
            stability_data = results['stability_analysis']
            models = [model for model in stability_data.keys() if model != 'comparison']
            
            if not models:
                return None
            
            # ç¨³å®šæ€§ç»´åº¦
            dimensions = [
                ('data_distribution_stability', 'average_stability_score', 'æ•°æ®åˆ†å¸ƒç¨³å®šæ€§'),
                ('noise_robustness', 'average_robustness_score', 'å™ªå£°é²æ£’æ€§'),
                ('temporal_stability', 'stability_score', 'æ—¶é—´ç¨³å®šæ€§'),
                ('cross_validation_stability', 'inter_run_stability', 'äº¤å‰éªŒè¯ç¨³å®šæ€§')
            ]
            
            fig = go.Figure()
            
            for model in models:
                if model in stability_data and 'overall_stability_score' in stability_data[model]:
                    scores = []
                    labels = []
                    
                    for dim_key, score_key, label in dimensions:
                        if (dim_key in stability_data[model] and 
                            isinstance(stability_data[model][dim_key], dict)):
                            
                            if score_key == 'inter_run_stability':
                                # ç‰¹æ®Šå¤„ç†äº¤å‰éªŒè¯ç¨³å®šæ€§
                                if ('inter_run_stability' in stability_data[model][dim_key] and
                                    'stability_score' in stability_data[model][dim_key]['inter_run_stability']):
                                    score = stability_data[model][dim_key]['inter_run_stability']['stability_score']
                                else:
                                    score = 0
                            else:
                                score = stability_data[model][dim_key].get(score_key, 0)
                            
                            scores.append(score)
                            labels.append(label)
                    
                    if scores:
                        # æ·»åŠ ç¬¬ä¸€ä¸ªç‚¹åˆ°æœ«å°¾ä»¥é—­åˆé›·è¾¾å›¾
                        scores.append(scores[0])
                        labels.append(labels[0])
                        
                        fig.add_trace(go.Scatterpolar(
                            r=scores,
                            theta=labels,
                            fill='toself',
                            name=model
                        ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 1]
                    )),
                showlegend=True,
                title="æ¨¡å‹ç¨³å®šæ€§é›·è¾¾å›¾"
            )
            
            chart_path = os.path.join(self.charts_dir, "stability_radar.html")
            fig.write_html(chart_path)
            
            return chart_path
            
        except Exception as e:
            print(f"åˆ›å»ºç¨³å®šæ€§é›·è¾¾å›¾å¤±è´¥: {str(e)}")
            return None
    
    def _create_performance_stability_scatter(self, results: Dict) -> str:
        """åˆ›å»ºæ€§èƒ½-ç¨³å®šæ€§æ•£ç‚¹å›¾"""
        
        try:
            if ('integrated_analysis' not in results or 
                'model_rankings' not in results['integrated_analysis']):
                return None
            
            rankings = results['integrated_analysis']['model_rankings']
            
            if 'by_overall' not in rankings:
                return None
            
            models = []
            performance_scores = []
            stability_scores = []
            overall_scores = []
            
            for model, scores in rankings['by_overall']:
                models.append(model)
                performance_scores.append(scores['performance_score'])
                stability_scores.append(scores['stability_score'])
                overall_scores.append(scores['overall_score'])
            
            fig = go.Figure()
            
            fig.add_trace(go.Scatter(
                x=performance_scores,
                y=stability_scores,
                mode='markers+text',
                text=models,
                textposition="top center",
                marker=dict(
                    size=[score * 50 + 10 for score in overall_scores],  # æ°”æ³¡å¤§å°è¡¨ç¤ºç»¼åˆè¯„åˆ†
                    color=overall_scores,
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="ç»¼åˆè¯„åˆ†")
                ),
                name='æ¨¡å‹'
            ))
            
            fig.update_layout(
                title="æ¨¡å‹æ€§èƒ½-ç¨³å®šæ€§æ•£ç‚¹å›¾",
                xaxis_title="æ€§èƒ½è¯„åˆ†",
                yaxis_title="ç¨³å®šæ€§è¯„åˆ†",
                showlegend=False
            )
            
            # æ·»åŠ è±¡é™åˆ†å‰²çº¿
            fig.add_hline(y=np.median(stability_scores), line_dash="dash", line_color="gray")
            fig.add_vline(x=np.median(performance_scores), line_dash="dash", line_color="gray")
            
            chart_path = os.path.join(self.charts_dir, "performance_stability_scatter.html")
            fig.write_html(chart_path)
            
            return chart_path
            
        except Exception as e:
            print(f"åˆ›å»ºæ€§èƒ½-ç¨³å®šæ€§æ•£ç‚¹å›¾å¤±è´¥: {str(e)}")
            return None
    
    def _create_resource_usage_trend_chart(self) -> str:
        """åˆ›å»ºèµ„æºä½¿ç”¨è¶‹åŠ¿å›¾"""
        
        try:
            if not self.performance_monitor.resource_logs:
                return None
            
            timestamps = []
            cpu_usage = []
            memory_usage = []
            process_memory = []
            
            for log in self.performance_monitor.resource_logs:
                timestamps.append(datetime.fromisoformat(log['timestamp']))
                cpu_usage.append(log['system']['cpu_percent'])
                memory_usage.append(log['system']['memory_percent'])
                process_memory.append(log['process']['memory_mb'])
            
            fig = make_subplots(
                rows=3, cols=1,
                subplot_titles=('CPUä½¿ç”¨ç‡ (%)', 'ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡ (%)', 'è¿›ç¨‹å†…å­˜ä½¿ç”¨ (MB)'),
                shared_xaxes=True
            )
            
            # CPUä½¿ç”¨ç‡
            fig.add_trace(
                go.Scatter(x=timestamps, y=cpu_usage, name='CPUä½¿ç”¨ç‡', 
                          line=dict(color='red')),
                row=1, col=1
            )
            
            # ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡
            fig.add_trace(
                go.Scatter(x=timestamps, y=memory_usage, name='ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡', 
                          line=dict(color='blue')),
                row=2, col=1
            )
            
            # è¿›ç¨‹å†…å­˜ä½¿ç”¨
            fig.add_trace(
                go.Scatter(x=timestamps, y=process_memory, name='è¿›ç¨‹å†…å­˜ä½¿ç”¨', 
                          line=dict(color='green')),
                row=3, col=1
            )
            
            fig.update_layout(
                title_text="èµ„æºä½¿ç”¨è¶‹åŠ¿",
                showlegend=False,
                height=800
            )
            
            chart_path = os.path.join(self.charts_dir, "resource_usage_trend.html")
            fig.write_html(chart_path)
            
            return chart_path
            
        except Exception as e:
            print(f"åˆ›å»ºèµ„æºä½¿ç”¨è¶‹åŠ¿å›¾å¤±è´¥: {str(e)}")
            return None
    
    def generate_comprehensive_report(self, results: Dict) -> str:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        
        try:
            report_content = self._create_report_content(results)
            
            report_path = os.path.join(self.reports_dir, 
                                     f"performance_stability_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html")
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            print(f"ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            return report_path
            
        except Exception as e:
            print(f"ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return None
    
    def _create_report_content(self, results: Dict) -> str:
        """åˆ›å»ºæŠ¥å‘Šå†…å®¹"""
        
        html_content = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>æ¨¡å‹æ€§èƒ½ä¸ç¨³å®šæ€§ç»¼åˆåˆ†ææŠ¥å‘Š</title>
            <style>
                body {{
                    font-family: 'Microsoft YaHei', Arial, sans-serif;
                    line-height: 1.6;
                    margin: 0;
                    padding: 20px;
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background-color: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 0 20px rgba(0,0,0,0.1);
                }}
                h1, h2, h3 {{
                    color: #333;
                    border-bottom: 2px solid #4CAF50;
                    padding-bottom: 10px;
                }}
                .summary-box {{
                    background-color: #e8f5e8;
                    padding: 20px;
                    border-radius: 8px;
                    margin: 20px 0;
                    border-left: 5px solid #4CAF50;
                }}
                .warning-box {{
                    background-color: #fff3cd;
                    padding: 20px;
                    border-radius: 8px;
                    margin: 20px 0;
                    border-left: 5px solid #ffc107;
                }}
                .error-box {{
                    background-color: #f8d7da;
                    padding: 20px;
                    border-radius: 8px;
                    margin: 20px 0;
                    border-left: 5px solid #dc3545;
                }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                }}
                th, td {{
                    border: 1px solid #ddd;
                    padding: 12px;
                    text-align: left;
                }}
                th {{
                    background-color: #f2f2f2;
                    font-weight: bold;
                }}
                .metric {{
                    display: inline-block;
                    margin: 10px;
                    padding: 15px;
                    background-color: #f8f9fa;
                    border-radius: 8px;
                    border: 1px solid #dee2e6;
                }}
                .chart-container {{
                    margin: 30px 0;
                    text-align: center;
                }}
                .recommendation {{
                    background-color: #d1ecf1;
                    padding: 15px;
                    border-radius: 8px;
                    margin: 10px 0;
                    border-left: 4px solid #17a2b8;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸš€ æ¨¡å‹æ€§èƒ½ä¸ç¨³å®šæ€§ç»¼åˆåˆ†ææŠ¥å‘Š</h1>
                
                <div class="summary-box">
                    <h3>ğŸ“Š æŠ¥å‘Šæ¦‚è¦</h3>
                    <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
                    <p><strong>åˆ†æèŒƒå›´:</strong> æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•ã€ç¨³å®šæ€§è¯„ä¼°ã€èµ„æºä½¿ç”¨ç›‘æ§</p>
                    <p><strong>è¯„ä¼°ç»´åº¦:</strong> å‡†ç¡®ç‡ã€è®­ç»ƒæ—¶é—´ã€é¢„æµ‹æ—¶é—´ã€å†…å­˜ä½¿ç”¨ã€ç¨³å®šæ€§è¯„åˆ†</p>
                </div>
                
                {self._generate_performance_section(results)}
                
                {self._generate_stability_section(results)}
                
                {self._generate_integrated_section(results)}
                
                {self._generate_recommendations_section(results)}
                
                <div class="summary-box">
                    <h3>ğŸ“ˆ æ€»ç»“</h3>
                    <p>æœ¬æŠ¥å‘Šé€šè¿‡ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå¤šç»´åº¦ç¨³å®šæ€§è¯„ä¼°ï¼Œä¸ºæ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–æä¾›äº†æ•°æ®æ”¯æŒã€‚
                    å»ºè®®æ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚ï¼Œåœ¨æ€§èƒ½ã€ç¨³å®šæ€§å’Œæ•ˆç‡ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ã€‚</p>
                </div>
                
                <footer style="margin-top: 50px; text-align: center; color: #666;">
                    <p>æŠ¥å‘Šç”±æ€§èƒ½ç›‘æ§å’Œç¨³å®šæ€§æµ‹è¯•ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ</p>
                </footer>
            </div>
        </body>
        </html>
        """
        
        return html_content
    
    def _generate_performance_section(self, results: Dict) -> str:
        """ç”Ÿæˆæ€§èƒ½åˆ†æéƒ¨åˆ†"""
        
        section = "<h2>ğŸ“Š æ€§èƒ½åˆ†æ</h2>"
        
        try:
            if ('performance_analysis' in results and 
                'benchmark' in results['performance_analysis']):
                
                benchmark_data = results['performance_analysis']['benchmark']
                
                section += "<h3>ğŸƒâ€â™‚ï¸ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ</h3>"
                section += "<table>"
                section += "<tr><th>æ¨¡å‹</th><th>å¹³å‡è®­ç»ƒæ—¶é—´(ç§’)</th><th>å¹³å‡é¢„æµ‹æ—¶é—´(ç§’)</th><th>å¹³å‡å‡†ç¡®ç‡</th><th>å‡†ç¡®ç‡æ ‡å‡†å·®</th></tr>"
                
                for model_name, data in benchmark_data.items():
                    if 'statistics' in data:
                        stats = data['statistics']
                        section += f"""
                        <tr>
                            <td>{model_name}</td>
                            <td>{stats['training_time']['mean']:.4f}</td>
                            <td>{stats['prediction_time']['mean']:.4f}</td>
                            <td>{stats['accuracy']['mean']:.4f}</td>
                            <td>{stats['accuracy']['std']:.4f}</td>
                        </tr>
                        """
                
                section += "</table>"
                
                # æ€§èƒ½è¶‹åŠ¿åˆ†æ
                if 'trends' in results['performance_analysis']:
                    trends = results['performance_analysis']['trends']
                    section += "<h3>ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ</h3>"
                    
                    if 'efficiency_ranking' in trends and 'best_model' in trends['efficiency_ranking']:
                        best_model = trends['efficiency_ranking']['best_model']
                        section += f"<div class='summary-box'>ğŸ† æ•ˆç‡æœ€ä¼˜æ¨¡å‹: <strong>{best_model}</strong></div>"
                    
                    if 'stability_ranking' in trends and 'most_stable_model' in trends['stability_ranking']:
                        stable_model = trends['stability_ranking']['most_stable_model']
                        section += f"<div class='summary-box'>ğŸ›¡ï¸ æœ€ç¨³å®šæ¨¡å‹: <strong>{stable_model}</strong></div>"
        
        except Exception as e:
            section += f"<div class='error-box'>âŒ æ€§èƒ½åˆ†æéƒ¨åˆ†ç”Ÿæˆå¤±è´¥: {str(e)}</div>"
        
        return section
    
    def _generate_stability_section(self, results: Dict) -> str:
        """ç”Ÿæˆç¨³å®šæ€§åˆ†æéƒ¨åˆ†"""
        
        section = "<h2>ğŸ”¬ ç¨³å®šæ€§åˆ†æ</h2>"
        
        try:
            if 'stability_analysis' in results:
                stability_data = results['stability_analysis']
                
                section += "<h3>ğŸ§ª ç¨³å®šæ€§æµ‹è¯•ç»“æœ</h3>"
                section += "<table>"
                section += "<tr><th>æ¨¡å‹</th><th>ç»¼åˆç¨³å®šæ€§è¯„åˆ†</th><th>æ•°æ®åˆ†å¸ƒç¨³å®šæ€§</th><th>å™ªå£°é²æ£’æ€§</th><th>æ—¶é—´ç¨³å®šæ€§</th></tr>"
                
                for model_name, data in stability_data.items():
                    if model_name != 'comparison' and 'overall_stability_score' in data:
                        overall_score = data['overall_stability_score']
                        
                        # æå–å„ç»´åº¦è¯„åˆ†
                        dist_score = data.get('data_distribution_stability', {}).get('average_stability_score', 'N/A')
                        noise_score = data.get('noise_robustness', {}).get('average_robustness_score', 'N/A')
                        temp_score = data.get('temporal_stability', {}).get('stability_score', 'N/A')
                        
                        section += f"""
                        <tr>
                            <td>{model_name}</td>
                            <td>{overall_score:.4f}</td>
                            <td>{dist_score if isinstance(dist_score, str) else f'{dist_score:.4f}'}</td>
                            <td>{noise_score if isinstance(noise_score, str) else f'{noise_score:.4f}'}</td>
                            <td>{temp_score if isinstance(temp_score, str) else f'{temp_score:.4f}'}</td>
                        </tr>
                        """
                
                section += "</table>"
                
                # ç¨³å®šæ€§å¯¹æ¯”åˆ†æ
                if 'comparison' in stability_data:
                    comparison = stability_data['comparison']
                    section += "<h3>âš–ï¸ ç¨³å®šæ€§å¯¹æ¯”åˆ†æ</h3>"
                    
                    if 'stability_categories' in comparison:
                        categories = comparison['stability_categories']
                        for category, models in categories.items():
                            if models:
                                section += f"<div class='metric'><strong>{category}:</strong> {', '.join(models)}</div>"
        
        except Exception as e:
            section += f"<div class='error-box'>âŒ ç¨³å®šæ€§åˆ†æéƒ¨åˆ†ç”Ÿæˆå¤±è´¥: {str(e)}</div>"
        
        return section
    
    def _generate_integrated_section(self, results: Dict) -> str:
        """ç”Ÿæˆé›†æˆåˆ†æéƒ¨åˆ†"""
        
        section = "<h2>ğŸ”„ é›†æˆåˆ†æ</h2>"
        
        try:
            if 'integrated_analysis' in results:
                integrated = results['integrated_analysis']
                
                # æœ€ä¼˜æ¨¡å‹é€‰æ‹©
                if 'optimal_model_selection' in integrated:
                    selection = integrated['optimal_model_selection']
                    section += "<h3>ğŸ† æœ€ä¼˜æ¨¡å‹é€‰æ‹©</h3>"
                    
                    for category, info in selection.items():
                        if isinstance(info, dict) and 'model' in info:
                            category_name = {
                                'best_overall': 'ç»¼åˆæœ€ä¼˜',
                                'best_performance': 'æ€§èƒ½æœ€ä¼˜',
                                'best_stability': 'ç¨³å®šæ€§æœ€ä¼˜',
                                'best_efficiency': 'æ•ˆç‡æœ€ä¼˜'
                            }.get(category, category)
                            
                            section += f"<div class='summary-box'><strong>{category_name}:</strong> {info['model']}</div>"
                
                # æ€§èƒ½-ç¨³å®šæ€§çŸ©é˜µ
                if 'performance_stability_matrix' in integrated:
                    matrix = integrated['performance_stability_matrix']
                    section += "<h3>ğŸ“Š æ€§èƒ½-ç¨³å®šæ€§çŸ©é˜µ</h3>"
                    
                    for category, models in matrix.items():
                        if models:
                            category_name = {
                                'high_performance_high_stability': 'é«˜æ€§èƒ½é«˜ç¨³å®šæ€§',
                                'high_performance_low_stability': 'é«˜æ€§èƒ½ä½ç¨³å®šæ€§',
                                'low_performance_high_stability': 'ä½æ€§èƒ½é«˜ç¨³å®šæ€§',
                                'low_performance_low_stability': 'ä½æ€§èƒ½ä½ç¨³å®šæ€§'
                            }.get(category, category)
                            
                            section += f"<div class='metric'><strong>{category_name}:</strong> {', '.join(models)}</div>"
        
        except Exception as e:
            section += f"<div class='error-box'>âŒ é›†æˆåˆ†æéƒ¨åˆ†ç”Ÿæˆå¤±è´¥: {str(e)}</div>"
        
        return section
    
    def _generate_recommendations_section(self, results: Dict) -> str:
        """ç”Ÿæˆå»ºè®®éƒ¨åˆ†"""
        
        section = "<h2>ğŸ’¡ å»ºè®®ä¸æ€»ç»“</h2>"
        
        try:
            if 'recommendations' in results:
                recommendations = results['recommendations']
                section += "<h3>ğŸ¯ å…·ä½“å»ºè®®</h3>"
                
                for rec in recommendations:
                    section += f"<div class='recommendation'>{rec}</div>"
        
        except Exception as e:
            section += f"<div class='error-box'>âŒ å»ºè®®éƒ¨åˆ†ç”Ÿæˆå¤±è´¥: {str(e)}</div>"
        
        return section


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # å®šä¹‰æ¨¡å‹
    models = {
        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        'LogisticRegression': LogisticRegression(random_state=42),
        'SVM': SVC(random_state=42)
    }
    
    # åˆ›å»ºæ€§èƒ½ç¨³å®šæ€§ç³»ç»Ÿ
    system = PerformanceStabilitySystem("performance_stability_output")
    
    # æ‰§è¡Œç»¼åˆåˆ†æ
    results = system.comprehensive_analysis(
        models, X_train, X_test, y_train, y_test
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = system.generate_comprehensive_report(results)
    
    print(f"âœ… åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šè·¯å¾„: {report_path}")
    print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {system.charts_dir}")
    print(f"ğŸ“„ æŠ¥å‘Šç›®å½•: {system.reports_dir}")